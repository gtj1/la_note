### 从 $V$ 到 $V$ —— 特征值和行列式

> 许多教科书引入行列式的方式莫名其妙

#### 线性变换会在哪里出现

先回顾什么是线性变换：我们称一个把线性空间映射到自身的线性映射称为线性变换，即一个 $V$ 上的线性变换是一个映射 $\varphi: V\to V$

线性变换作用在一个空间上，当我们需要输入一个向量并且输出同一个空间内的向量时线性变换就出现了. 比如，当一个光子碰到一面镜子并且反射的时候，镜子接受了它的动量并且给了它一个新的动量作为输出，所以镜子可以看作给出了动量所处空间的线性变换. 对于一个线性递推数列 $a:\mathbb{R}\to\mathbb{R}$ ( 递推公式为 $a_{n+m}=k_0 a_n+k_1a_{n+1}+\cdots+k_{m-1}a_{n+m-1}$ ). 去除其前 $t$ 项并且把每一个 $a_{n+t}$ 向前移动到 $a_n$ 并不改变它的递推公式. 注意到所有满足递推公式的数列组成一个线性空间，且去除前 $t$ 项的操作是线性的 （去除*两个数列的和数列* 的前几项等价于两个去除了前几项的数列的和）所以去除前 $t$ 项的操作是线性的并且把数列映射到数列. 这是一个满足递推公式的数列空间上的线性变换. 设去除前 $t$ 项的操作为 $R_t$. 则容易验证 $R_{t_1+t_2}=R_{t_1}\circ R_{t_2}$. 此外，行变换和列变换作用在矩阵空间上，我们可以使用它们消元或者完成正交化.

到处都有线性变换，它们通常会乘在一起或者和自身相乘（这意味着取一个线性变换的幂）

#### 没有哪个基更加特殊 —— 为什么我们需要不变量

在从第三章开始的针对一般的（特别是有限维）线性空间的讨论中，我们通过把抽象线性空间中的向量"翻译"为它们在某个基底下得到的 $\mathbb{R}^n$ 中的坐标. 这让我们感到更加亲切，也更容易处理. 我们已经讨论过如何在两个基底之间转换，也学会了如何在给定的内积下从任何一个基底得到一个正交基. 然而，这一切都建立在一个前提下：你不会为了如何选择基底而感到困难. 然而，如果空间中并没有什么条件可以告诉你你该怎么选取基底导致你产生选择困难，你解决问题就会困难起来. 这并不完全是一个笑话，数学家们总是想要做出"好"的选择并且最终加入了选择公理告诉我们存在这样一个选择函数，它总是可以从一个非空集合中选择一个元素. (这并非本文的话题，如果你感兴趣你可以自行搜索以了解更多)

通常有很多种方式为一个向量空间选择基底并且研究映射的性质. 但是随着基向量的变化，许多性质，例如某个向量的坐标也会随之变化. 而那些不随基底变化的性质就被称作不变量. 找到和基底选取无关的性质通常很重要，因为这样的性质反映了映射的本质属性. 换句话说，当 Alice 和 Bob 从不同角度看一个空间时，他们能得到什么共同结论?

在继续讨论这个话题之前，我们需要回忆一些我们学过的关于(有限维)线性空间或者其上的线性映射的不变量：

1. 线性相关性：一个向量组是否线性相关与基底选取无关
2. 维数：一个空间的维数不会随着基底的选取而变化，所以基底可能不同，但是基向量的数量一定相同，这就是空间的维数
3. 秩：秩反映了一个线性映射的像的维数. 对于一个 $\mathbb{R}^n$ 到其它空间的映射，这个映射作为一个向量组，即使向量组本身可能线性相关，但是它的秩告诉我们一个向量组中有多少个"有用"的向量
4. 零化度：此前我没提过这个概念，一个映射 $\varphi$ 的零化度就是 $\dim\operatorname{Ker}\varphi$. 它通过测量核的大小表明有"多少"向量会被映射到 $0$. 对于一个从 $\mathbb{R}^n$ 到某个空间的映射，即一个向量组，这告诉我们有多少种"不同"的方式让它的线性组合为 $0$ (如果 $\alpha$是一个向量组，其中 $\alpha_2-2\alpha_1=0$ 且 $\alpha_3-2\alpha_2=0$，这时我们不认为 $\alpha_3-4\alpha_1$ 是一个"不同的"线性组合因为它可以看作前面两种组合的线性组合），由于 $\dim\operatorname{Ker}\varphi=n-r$. 向量组的零化度可以视作其中"无用"的向量的数量. 所以线性代数基本定理又被称为 秩-零化度定理. 对于 $\mathbb{R}^n\to\mathbb{R}^m$ 的情况，秩和零化度分别对应阶梯头和自由变量的数量.

可以证明对于 $\varphi: V_1\to V_2$, 其中 $\dim V_1=n$, $\dim V_2=m$, 秩 $r$, 与维数 $m$, $n$ 可以生成所有关于这个线性映射的所有不变量. 这可以通过把所有矩阵都归约到一个标准型来实现. 此处"生成"的意思是所有的其它不变量都可以根据它们来计算. 例如，零化度可以通过 $\dim\operatorname{Ker}\varphi=n-r$ 计算. 证明这一点并不难但不是我们现在要讨论的话题. 一定程度上，这是因为你可以自由地选择 $V_1$ 和 $V_2$ 的基，然而，当我们为线性映射加上其它条件时，可能会出现新的不变量. 在接下来的内容中，我们把线性映射限制在一个空间上，变成 $\varphi: V\to V$ 并且谈论这是如何带来新性质的.

#### 体积的变化 —— 一步步构建起行列式

##### 第一步 —— 找到线性映射的额外不变量

###### 先自己思考一下

为了找到线性映射有什么额外的性质，首先考虑一些简单的例子. 比如，首先考虑 $\mathbb{R}^n$. 但是你要假设你忘掉了它原本的坐标系统，认为你的基向量是随便选取的，它不够自然. 第一个例子是 $\mathbb{R}^1=\mathbb{R}$, 此时所有的线性变换都是一个缩放，只由一个数字 $k$ 决定，即 $\varphi(x)=[k](x)$. 在这里讨论线性无关都没有意义. 所以接下来让我们直接继续考虑 $\mathbb{R}^2$. 下图和第二章初见矩阵的图是相同的图. 看看图并想想，即使我们随便选择一个基，我们可以知道什么关于它的性质.

<center><img src="https://gtj-10032.oss-cn-hangzhou.aliyuncs.com/images/2-1.svg"/></center>

###### 几何量

首先，我们不可能讨论长度、角度或者面积的确定大小，因为通常来讲这个空间不一定定义了度量，不同的度量会给出不同的结果. 所以这些结果实际上是度量而非空间本身的性质. 然而我们可以讨论向量关于原来向量的变化. 在一般的空间中，只有当两个向量线性的时候我们才能拿它们作比较. 所以如果我们要这么干我们就需要找到一些向量 $v$ 使得 $\varphi(v)$ 和 $v$ 共线，换言之，$\varphi(v)=\lambda v$. 这样的值 $\lambda$ 被称作特征值，对应的向量被称作特征向量. 我们会在之后详细讨论这些内容.

###### 关于空间中的面积

虽然我们不能讨论向量的长度，在一个 $2$ 维空间中，我们总是可以讨论"面积"的变化. 你可能在想，我们都没有度量，哪来的面积呢？好消息是，可以证明面积变化的比例与度量的选取无关，这可以通过用正方形近似并且证明所有的正方形缩放方式相同来证明. 如果你对此感到困惑，请移步 3blue1brown 的视频再回到本文，视频链接：

https://www.bilibili.com/video/BV1ys411472E?p=7

##### 第二步 —— 定义二维空间的有向面积

###### 预备知识

什么是定向？这并不难理解。简而言之，定向就是字母 '$\text{p}$' 和 '$\mathrm{b}$' 之间的差异。或者它是你左手和右手之间的差异。如果你在一张有弹性的纸上写字母 '$\mathrm{p}$'，无论如何旋转或拉伸纸张，都无法将其变成字母 '$\mathrm{b}$'。尽管你可以翻转纸张，将 '$\mathrm{p}$' 变成 '$\mathrm{b}$'，但这个动作不在原始的纸面二维空间内，所以可以改变定向。就手的例子来说，无论你如何旋转或移动左手，它都不会变得和右手一样。在化学中，这种现象通常被称为手性。如果存在一个$4$维空间，理论上，$4$维空间中的生物可以将你“翻转”回到我们的$3$维空间。但在我们的空间中，这是不可能的。

可以为任何$n$维空间定义一个"定向"。注意到通过翻转垂直轴，字母'$\mathrm{p}$'变成了具有不同定向的字母'$\mathrm{b}$'，但接下来通过翻转水平轴，它又变成了字母'$\mathrm{d}$'，可以旋转$180\degree$与'$\mathrm{p}$'重叠。因此，如果我们翻转偶数个轴，定向保持不变，而当我们翻转奇数个轴时，定向会改变。这看起来和$(-1)^k$很像，当$k$是偶数时得到结果$1$，当$k$是奇数时得到结果$-1$。

然后让我们谈谈轴的交换。如果我们绕着直线$y=x$翻转，这会交换水平和垂直的坐标轴，字母'$\mathrm{p}$'会变成顺时针旋转了$90\degree$的字母'$\mathrm{b}$'，它与'$\mathrm{p}$'有不同的定向。另一次翻转或轴的改变会将其定向还原。因此，似乎只有两种定向，就像我们有两只不同的手一样。如果一个物体的定向改变了偶数次，那么与原始定向相比，它是正定向的$(+1)$。如果一个物体的定向改变了奇数次，那么与原始定向相比，它是负定向的$(-1)$。

我们经常使用的坐标系是右手坐标系，因此通常认为左手坐标系为负定向。

###### 为什么要有向面积

> 虽然这一小节我们在讨论 $\mathbb{R}^2$, 之后我们将讨论的会是一个一般的二维线性空间

假设 $\mathbb{R}^2$ 中有一个平行四边形，其顶点为
$$
A(0,0),B(a,c),C(a+b,c+d),D(b,d)
$$
这是向量 $\alpha_1=(a,c)$ 和 $\alpha_2=(b,d)$ 根据平行四边形法则确定的平行四边形，请自己画个图并且计算它的面积. $\alpha_2$ 在 $\alpha_1$ 的左侧或者右侧会引出不同的结果，前者的面积为 $S=ad-bc$, 而后者的为 $S=bc-ad$. 这个公式可以总结为 $S=|ad-bc|$. 此外我们还需要排除 $\alpha_1$ 与 $\alpha_2$ 共线的情况，这让它不可能成为平行四边形.

定义有向面积的主要原因之一是，它允许负面积作为负定向出现且允许 $0$ 作为共线出现. 其中通过符号来确定定向只是一个副产物，不过这也许也有其作用. 例如，如果我们只是测量 '$\mathrm{p}$' 和 '$\mathrm{b}'$ 上点间的距离，我们会发现它们全等，所以不能区分出它们. 然而通过计算给定两条轴之间的有向面积，我们可以将二者分开. 有向面积其实就是面积加上了一个符号来表示定向，它使用 $ad-bc$ 代替了 $|ad-bc|$ 并且告诉我们更多关于平行四边形的信息. 唯一的缺陷是，我们不能自由地交换 $\alpha_1$ 和 $\alpha_2$，因为这会改变定向. 虽然有向面积有一些缺陷，但瑕不掩瑜，它仍然是一个好东西. 我们将会像之前定义其它概念一样，用几条规则来定义有向面积，解释这些规则的合理性并且得到相关性质.

###### 有向面积遵循的规则和基础性质

> 这几段定义了有向面积并且给出了一些解释

为了正式定义有向面积（或称作有符号面积），我们将会从最简单的对象：平行四边形开始. 这里的平行四边形指的并非我们平常的平行四边形，而是一个向量组 $\alpha$ 中的 $\alpha_1$ 和 $\alpha_2$ 由平行四边形定则确定的四边形. 有向面积应该满足如下的性质：

有向面积以向量组 $\alpha$ 作为输入，其输出满足：

1. 把平行四边形的一条边缩放一个因子 $k$ 且不改变其它边会让最终的面积变为原来的 $k$ 倍，即
   $$
   f(k\alpha_1, \alpha_2)=f(\alpha_1, k\alpha_2)=kf(\alpha_1,\alpha_2)
   $$
   注：当 $k$ 为负数时，平行四边形的定向会发生改变，所以有向面积会变号，考虑定向有助于去掉绝对值，用一种更加线性的方法来考虑问题

2. 如果一个平行四边形可以通过适当的切割和平移得到其它的四边形，那么这两个四边形的面积应该是相同的，然而我们不认为交换两个向量得到的平行四边形是相同的四边形.

如下的图展示了上面的两条规则，$S$ 是 $\left[\begin{array}c \alpha_1 & \alpha_2 \end{array}\right]$ 的有向面积：

<img src="5-3.svg"/>

> 这一段讨论了有向面积在列加减中不变

右上角的图告诉我们对常数 $k$, 当把 $\alpha_1$ 改为 $\alpha_1'=\alpha_1+k\alpha_2$ 时，平行四边形改变的方式可以视作把一部分切下来然后平移到另一边得到平行四边形 $\left[\begin{array}c \alpha_1' & \alpha_2 \end{array}\right]$. 所以 $f(\alpha_1,\alpha_2)=f(\alpha_1+k\alpha_2, \alpha_2)$. 同样地，可以证明把 $\alpha_1$ 的某个倍数加到 $\alpha_2$ 上时结果也成立.

> 接下来的段落承接上一段，证明两个共线向量的有向面积为 $0$. 特别地，这对一个向量和它自身成立.

设 $\alpha_1$ 和 $\alpha_2$ 共线，例如 $\alpha_2=k\alpha_1$, 则
$$
f(\alpha_1,\alpha_2)=f(\alpha_1, k\alpha_1)=f(\alpha_1, k\alpha_1-k\alpha_1)=f(\alpha_1, 0)=0f(\alpha_1, \alpha_1)=0
$$
所以对于两个共线向量，可以推出面积为 $0$. 这和我们的直觉一致，它的核不为 $0$, 把线性空间压缩到了一条线.

> 以下段落通过选取适当基底证明了有向面积的多线性

在第一个性质的注中，我提到了这让有向面积变得线性. 所以我们会期望 $f(\alpha_1, \alpha_2+\beta)=f(\alpha_1, \alpha_2)+f(\alpha_1, \beta)$. 证明分为两部分，如果 $\beta$ 和 $\alpha_2$ 都与 $\alpha_1$ 线性相关，则三项都为 $0$, 等式成立. 接下来假设其中一个与 $\alpha_1$ 线性无关，不妨设 $\alpha_2$ 与 $\alpha_1$ 线性无关，则 $\alpha$ 包含两个线性无关的向量，从而成为这个二维空间的一个基. 可以用基 $\alpha$ 的一个线性组合来表示 $\beta$. 设 $\beta=x_1\alpha_1+x_2\alpha_2$. 则左侧的结果为
$$
f(\alpha_1,\alpha_2+\beta)=f(\alpha_1,x_1\alpha_1+(x_2+1)\alpha_2)=f(\alpha_1,(x_2+1)\alpha_2)=(x_2+1)f(\alpha_1,\alpha_2)
$$
右侧的结果是
$$
f(\alpha_1,\alpha_2)+f(\alpha_1,x_1\alpha_1+x_2\alpha_2)=f(\alpha_1,\alpha_2)+f(\alpha_1,x_2\alpha_2)=(x_2+1)f(\alpha_1,\alpha_2)
$$

> 这一段讨论为什么有向面积在一定程度上比原始的面积有更好的性质

在以上的证明中，我们不关系 $x_1, x_2$ 或者 $x_2+1$ 的具体符号，因为有向面积遵循的规则支持我们直接把这些系数提出来. 这对计算来说难道不方便吗？当 $\alpha_2$ 在 $\alpha_1$ 左侧时，结果是正的. 而当 $\alpha_2$ 在 $\alpha_1$ 右侧时，结果是负的. 在一定程度上，有向面积比面积本身更加自然. 下面的图是一个图像证明（这个图只是一个二维的图，不要误以为它是三维的. 它只在 $x_2>0$ 的时候才能有效论证结果成立）

<img src="5-4.svg"/>

###### 有向面积的新符号

> 本段将 $u\wedge v$ 作为 $f(u,v)$ 的别名引入. 这就像我们使用矩阵代替 $\mathbb{R}^n\to \mathbb{R}^m$ 时做的一样. 主要目的是为了简化记号. 请不要认为它满足交换律，否则当你看到反交换的时候会感到非常困惑.

为了方便，从现在开始我将会用 $u\wedge v$ 来表示 $f(u, v)$. 根据前面的段落，我们已经证明了 $f$ 的多线性，用 $\wedge$ 来写就是 $(k_1u_1+k_2u_2)\wedge v=k_1u_1\wedge v+k_2u_2\wedge v$ 和 $u\wedge (k_1v_1+k_2v_2)=k_1u\wedge v_1+k_2u\wedge v_2$ (双线性). 此外我们发现把一个向量的倍数加到其它向量上并不改变结果，这与双线性可以共同推出
$$
\begin{aligned}
0&=(u+v)\wedge(u+v)\\
&=u\wedge(u+v)+v\wedge (u+v)\\
&=u\wedge v+v\wedge u\\
&\Rightarrow v\wedge u=-u\wedge v
\end{aligned}
$$
你完全可以用 $f$ 的记号来描述这些，但对我来讲，写这个会节省时间并且让它更加清晰. 既然 $u$ 和 $v$ 是任意的，这表明交换两个变量位置会导致计算的结果符号改变. 这被称作反交换性，也是上文一直强调交换两个向量不同于原本向量的原因.

> 这一段为有向面积找了一个单位并证明有向面积构成一维线性空间

在前面的讨论中，我们并未讨论平行四边形面积的具体值是什么，这是因为如果你把面积函数 $f$ 乘以某个常数，它应该满足的规则仍然成立，所以我们只能相对于某个"单位"测量面积. 只有当你选择了单位，例如 $\mathrm{m}^2$, 你才能讨论我们空间中什么东西的面积并且得到它(相对于这个单位)的值. 任意选一个平行四边形，设它由基 $\alpha$ 定义（这意味着面积非 $0$，所以可以作为一个单位），不妨将其有向面积作为一个单位 $\alpha_1\wedge \alpha_2$. 设 $\beta_1=a\alpha_1+c\alpha_2$, $\beta_2=b\alpha_1+d\alpha_2$, 则由 $\beta_1$ 和 $\beta_2$ 定义的平行四边形面积为
$$
\begin{aligned}
&\beta_1\wedge\beta_2\\
=&(a\alpha_1+c\alpha_2)\wedge (b\alpha_1+d\alpha_2)\\
=&a\alpha_1\wedge (b\alpha_1+d\alpha_2)+c\alpha_2\wedge (b\alpha_1+d\alpha_2)\\
=&a\alpha_1\wedge b\alpha_1+a\alpha_1\wedge d\alpha_2+c\alpha_2 \wedge b\alpha_1+c\alpha_2\wedge d\alpha_2\\
=&ab(\alpha_1\wedge \alpha_1)+ad(\alpha_1\wedge\alpha_2)+bc(\alpha_2\wedge\alpha_1)+cd(\alpha_2+\alpha_2)\\
=&ad(\alpha_1\wedge\alpha_2)+bc(-\alpha_1\wedge\alpha_2)\\
=&(ad-bc)(\alpha_1\wedge\alpha_2)
\end{aligned}
$$
所以任意的有向面积都可以写成  $\alpha_1\wedge \alpha_2$ 的一个倍数，所以有向面积的空间是一个一维向量空间，每一个线性变换都可以用一个缩放因子表示（即具有形式 $\varphi(\mathrm{面积})=k(\mathrm{面积})$. 为了了解线性变换 $\varphi$ 作用在有向面积上的结果，我们只需要观察某个平行四边形 $\alpha$ 的面积 $S$ 并且找到改变后的面积 $\varphi(S)$. 把它们相处就得到了系数 $k=\varphi(S)/S$. 然后可以得到 $\varphi$ 作用在有向面积上是一个正比例函数，比例为 $k$.

注：一般来讲向量相除不是好的运算，但是在一维空间中是可以的.

##### 第三步 —— 定义二维的行列式

###### 行列式遵循的基本规则

> 注：Gabriel Cramer 在18世纪引入了行列式，用于解线性方程，英文中"determinant"和"determine"是同源词，它决定了矩阵的一些重要性质. 然而中文的"行列式"可谓是一个奇怪的翻译，暂时把它接受下来就好了.

如果有映射 $\varphi:V\to V$, 其中 $V$ 是一个二维向量空间，它给出了有向面积所属线性空间上的线性映射. 记 $\det\varphi=\varphi(S)/S$ 并称之为 $\varphi$ 的行列式. 这个结果对每个非零的有向面积都相同，所以是良定义的. 所以我们可以随意选取一个基底 $\alpha$ 并且继续用它计算（因为基底不影响结果的选取）

<img src="5-2.svg"/>

设 $\alpha$ 是一个基，$\varphi$ 具有矩阵表示 $A$. 也就是说 $\varphi(\alpha)=\alpha A$. 我们可以通过 $A$ 在基底 $\alpha$ 下的行列式为 $\det_{\alpha}(A)=\det(\varphi)=\det(\alpha A\alpha^{-1})$. ( 之后我会证明矩阵的行列式与基底无关，所以可以直接写成 $\det A$ ). 更明确地说，它是这样定义的
$$
\det_{\alpha}\left[\begin{array}c x & y \end{array}\right]=\dfrac{\alpha(x)\wedge\alpha(y)}{\alpha_1\wedge\alpha_2}
$$

> 接下来的段落将会把有向面积的结果翻译为矩阵行列式的结果并且对矩阵行列式得到相同的结果 $ad-bc$

我们已经在一些规则的基础上证明了两条重要的性质，现在我们希望得到行列式的性质，此外还要加上另外一条.

1. 多线性

   对于第一个变量，

   $(k_1u_1+k_2u_2)\wedge v=k_1u_1\wedge v + k_2u_2\wedge v$

   如果我们把 $u$ 和 $v$ 用基底 $\alpha$ 下的坐标来表示，我们有
   $$
   \begin{aligned}
   &\alpha(k_1 x_1+k_2 x_2)\wedge\alpha(y)\\
   =&(k_1\alpha(x_1)+k_1\alpha(x_2))\wedge \alpha(y)\\
   =&k_1\alpha(x_1)\wedge \alpha(y)+k_2\alpha(x_2)\wedge\alpha(y)
   \end{aligned}
   $$
   因为我们定义 $\det_{\alpha}A=\det \varphi$ 为面积的比例，这意味着
   $$
   \det_{\alpha}\left[\begin{array}c x & y \end{array}\right]=\dfrac{\alpha(x)\wedge\alpha(y)}{\alpha_1\wedge\alpha_2}
   $$
   所以 $\alpha(x)\wedge\alpha(y)=(\det_{\alpha}\left[\begin{array}c x & y \end{array}\right])(\alpha_1\wedge\alpha_2)$

   有向面积对第一个变量线性可以写作
   $$
   (\det_{\alpha}\left[\begin{array}c k_1x_1+k_2x_2 & y \end{array}\right])(\alpha_1\wedge\alpha_2)=\\
   (k_1\det_{\alpha}\left[\begin{array}c x_1 & y \end{array}\right])(\alpha_1\wedge\alpha_2)+(k_2\det_{\alpha}\left[\begin{array}c x_2 & y \end{array}\right])(\alpha_1\wedge\alpha_2)
   $$
   其中 $\alpha_1\wedge \alpha_2$ 可以从等式两边消去（因为它们不共线，所以面积不为 $0$）
   $$
   \det_{\alpha}\left[\begin{array}c k_1 x_1+k_2 x_2 & y \end{array}\right]=k_1\det_{\alpha}\left[\begin{array}c x_1 & y \end{array}\right]+k_2\det_{\alpha}\left[\begin{array}c x_2 & y \end{array}\right]
   $$
   同理可证
   $$
   \det_{\alpha}\left[\begin{array}c x & k_1 y_1+k_2 y_2 \end{array}\right]=k_1\det_{\alpha}\left[\begin{array}c x & y_1 \end{array}\right]+k_2\det_{\alpha}\left[\begin{array}c x & y_2 \end{array}\right]
   $$
   所以行列式对每个变量线性，但是注意：
   $$
   \det_{\alpha}\left[\begin{array}c x_1+x_2 & y_1+y_2 \end{array}\right]\ne\det_{\alpha}\left[\begin{array}c x_1 & y_1 \end{array}\right]+\det_{\alpha}\left[\begin{array}c x_2 & y_2 \end{array}\right]
   $$
   对每个变量线性意味着你需要一个个地使用线性
   $$
   \det_{\alpha}\left[\begin{array}c x_1+x_2 & y_1+y_2 \end{array}\right]=\det_{\alpha}\left[\begin{array}c x_1 & y_1+y_2 \end{array}\right]+\det_{\alpha}\left[\begin{array}c x_2 & y_1+y_2 \end{array}\right]\\
   =\det_{\alpha}\left[\begin{array}c x_1 & y_1 \end{array}\right]+\det_{\alpha}\left[\begin{array}c x_1 & y_2 \end{array}\right]+\det_{\alpha}\left[\begin{array}c x_2 & y_1 \end{array}\right]+\det_{\alpha}\left[\begin{array}c x_2 & y_2 \end{array}\right]
   $$
   如果你希望展开它，你需要做的就像多项式乘法一样.

2. 反交换性.

   这意味着 $v\wedge u=-u\wedge v$. 仍然假设 $u=\alpha(x), v=\alpha(y)$，根据
   $$
   \alpha(x)\wedge\alpha(y)=(\det_{\alpha}\left[\begin{array}c x & y \end{array}\right])(\alpha_1\wedge\alpha_2)
   $$
   可以得到
   $$
   \begin{aligned}
   &v\wedge u\\
   =&(\det_{\alpha}\left[\begin{array}c y & x \end{array}\right])(\alpha_1\wedge\alpha_2)\\
   =&-u\wedge v\\
   =&-(\det_{\alpha}\left[\begin{array}c x & y \end{array}\right])(\alpha_1\wedge\alpha_2)
   \end{aligned}
   $$
   于是 $\det_{\alpha}\left[\begin{array}c y & x \end{array}\right]=-\det_{\alpha}\left[\begin{array}c x & y \end{array}\right]$

   交换向量会改变定向，在矩阵中就是说交换矩阵的两列改变行列式的符号.

   如果令 $x=y$, $\det_{\alpha}\left[\begin{array}c x & x \end{array}\right]=-\det_{\alpha}\left[\begin{array}c x & x \end{array}\right]$ 推出一个含有重复的列的矩阵行列式为 $0$ （此时 $\alpha A$ 中出现重复的向量，从而线性相关）

3. 对单位矩阵结果为 $1$

   虽然我们不能讨论一个面积的绝对大小，我们至少可以说 $\alpha$ 的面积等于它自己，也就是说
   $$
   1=\frac{\alpha_1\wedge \alpha_2}{\alpha_1\wedge \alpha_2}
   $$
   注意到 $I_2$ 作为单位矩阵作用在 $\alpha$ 上满足 $\alpha I_2=\alpha$, 所以
   $$
   \det_{\alpha}I_2=1
   $$

有了这些结果，我们可以纯粹地用矩阵和行列式的语言重新叙述 $ad-bc$ 的结果.
$$
\begin{aligned}
& \det_{\alpha}\left[\begin{array}c a & b \\ c & d \end{array}\right]\\
= & \det_{\alpha}\left[\begin{array}c a & b \\ 0 & d \end{array}\right]+\det_{\alpha}\left[\begin{array}c 0 & b \\ c & d \end{array}\right]\\
= & \det_{\alpha}\left[\begin{array}c a & b \\ 0 & 0\end{array}\right]+\det_{\alpha}\left[\begin{array}c a & 0 \\ 0 & d \end{array}\right]+\det_{\alpha}\left[\begin{array}c 0 & b \\ c & 0 \end{array}\right]+\det_{\alpha}\left[\begin{array}c 0 & 0 \\ c & d \end{array}\right]\\
= & ab\underset{\text{有重复的列}}{\underbrace{\det_{\alpha}\left[\begin{array}c 1 & 1 \\ 0 & 0 \end{array}\right]}}+
ad\underset{\text{单位阵}}{\underbrace{\det_{\alpha}\left[\begin{array}c 1 & 0 \\ 0 & 1 \end{array}\right]}}\\
&~~~~~~~~~~~~~~~~~~~~~~~~~~+bc\underset{\begin{array}c\text{列交换后}\\\text{的单位阵}\end{array}}{\underbrace{\det_{\alpha}\left[\begin{array}c 0 & 1 \\ 1 & 0 \end{array}\right]}}+cd\underset{\text{有重复的列}}{\underbrace{\det_{\alpha}\left[\begin{array}c 0 & 0 \\ 1 & 1 \end{array}\right]}}\\
=&ad-bc
\end{aligned}
$$
在此过程中，我们只用到了多线性，反交换和单位阵行列式为$1$的性质. 虽然这些性质是通过选取了某个基底来证明的，但是结果最终和特定的基底无关. 所以实际上我们并不需要 $\det$ 下的下标 $\alpha$, 而是可以直接写出 $\det \left[\begin{array}c a & b \\ c & d \end{array}\right]=ad-bc$

所以对于一个线性变换，$\det\varphi$ 是一个不变量，它不随基底而改变. 同样地，对于一个矩阵，$\det A$ 也是不变量，且等于它表示的任意线性映射的行列式. 这里"任意它表示的线性映射"是说 $A$ 在基 $\alpha$ 下可能表示映射 $\varphi$, 而在基 $\beta$ 下可能表示映射 $\psi$, 这些映射可能不同，但是它们的行列式一定相同.

##### 第四步 —— 把定义拓展到 $n$ 维

###### 把有向面积的定义推进

你还记得为什么我们选择了二维空间吗？是的，这是因为它相对简单，接下来我们将会推广这个结果并且得到 $n$ 维空间中的行列式. 但在此之前，我们需要找到有向面积的替代品.

在二维空间中, 一个基的定向由第二个向量在第一个向量的左侧还是右侧决定. 假设 $\alpha_2$ 在变动，除非 $\alpha_2$ 穿过 $\alpha_1$ 所在的直线，否则基的定向不会改变. 然而在加上一个维度之后我们可以轻易地翻转改变它的方向从而不需要跨过 $\alpha_1$. 因此在 $3$ 维空间中，你可能已经想到了，我们应该使用"有向体积"，这时平行四边形变成了平行六面体. 请回忆，有向面积构成一个向量空间. 如果我们称一个有向面积是一个 $2$-向量，并且称有向体积是一个 $3$-向量，那么对于 $n$ 维空间，应当有对应的 $n$-向量概念. 虽然对于我们来讲想象 $n$-向量如何变化，但是我们可以用 $2$-向量（有向面积）和 $3$-向量（有向体积）作为例子类比.

> 如果你感到困惑，在理解下文的时候把 $3$-向量这个词换成"有向体积"并且把 $u\wedge v\wedge w$ 换成 $f(u, v, w)$

在一个 $3$ 维空间中，$3$-向量应当遵循的规则应当包括：

1. 对每个变量线性，即

   $(k_1u_1+k_2u_2)\wedge v\wedge w=k_1 u_1\wedge v\wedge w+k_2 u_2\wedge v\wedge w$

   对 $v$ 和 $w$ 也有相同的公式

   （可以这么解释：从 $u_1+u_2$ 到平面 $v\wedge w$ 的有向距离等于 $u_1$ 与 $u_2$ 到平面 $v\wedge w$ 的有向距离的和）

2. 反交换性，如果交换两个向量的位置，结果改变符号.

   ${\color{red}u}\wedge {\color{lightgreen}v}\wedge {\color{blue}w}=-{\color{red}u}\wedge {\color{blue}w}\wedge {\color{lightgreen}v}=-{\color{lightgreen}v}\wedge {\color{red}u}\wedge {\color{blue}w}=- {\color{blue}w}\wedge {\color{lightgreen}v}\wedge{\color{red}u}$

根据以上两条规则，任何 $3$-向量都可以在给定的基 $\alpha$ 下分解，从而得到 $\alpha_1\wedge \alpha_2\wedge\alpha_3$ 的某个倍数，所以三维空间中的 $3$-向量组成的空间是一个一维空间.

###### 定义 $n$ 维行列式

请回忆：一个 $2\times 2$ 矩阵的行列式对每一列线性，满足反交换律且对单位矩阵取值为 $1$

仍然将 $n$-向量的比例定义为线性变换 $\varphi$ 的行列式. 关于 $n\times n$ 矩阵的行列式的结果仍然可以从 $n$-向量运算的方式得出. 这次我们直接通过三条规则来定义 $n\times n$ 矩阵的行列式：

矩阵的行列式是一个函数 $\det:\mathbb{R}^{n\times n}\to \mathbb{R}$，满足：

1. 多线性：对每一列线性，即对任意的 $1\le i\le n$,
   $$
   \det\left[\begin{array}c \cdots & k_1v_i + k_2v_i' & \cdots \end{array}\right]=
   k_1\det\left[\begin{array}c \cdots & v_i & \cdots \end{array}\right]
   +
   k_2\det\left[\begin{array}c \cdots & v_i' & \cdots \end{array}\right]
   $$

2. 反交换性：对任意 $i\ne j$,
   $$
   \det\left[\begin{array}c \cdots & v_i & \cdots & v_j & \cdots \end{array}\right]=-\det\left[\begin{array}c \cdots & v_j & \cdots & v_i & \cdots \end{array}\right]
   $$

3. $\det I_n=1$

接下来回忆我们证明过的更多关于有向面积的性质，例如，从一行中加或者减去另一行的若干倍，结果不变. 也就是说如果 $C_i-kC_j$ 把矩阵 $A$ 变为矩阵 $B$, 则 $\det A=\det B$. 证明如下，其中反交换性推出带有重复行的矩阵行列式为 $0$
$$
\begin{aligned}
&\det\left[\begin{array}c \cdots & v_i-kv_j & \cdots & v_j & \cdots \end{array}\right]\\
=&\det\left[\begin{array}c \cdots & v_i & \cdots & v_j & \cdots \end{array}\right]-k\left[\begin{array}c \cdots & v_j & \cdots & v_j & \cdots \end{array}\right]\\
=&\det\left[\begin{array}c \cdots & v_i & \cdots & v_j & \cdots \end{array}\right]-k\cdot 0\\
=&\det\left[\begin{array}c \cdots & v_i & \cdots & v_j & \cdots \end{array}\right]
\end{aligned}
$$
为了计算一个矩阵的行列式，我们只需要对它做列变换把它变成单位矩阵算出结果，又或者得到包含 $0$ 或重复列的矩阵得到 $0$，以两个三阶矩阵为例：
$$
\det\left[\begin{array}c 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{array}\right]\xlongequal[C_3-3C_1]{C_2-2C_1}
\det\left[\begin{array}c 1 & 0 & 0 \\ 4 & -3 & -6 \\ 7 & -6 & -12 \end{array}\right]\xlongequal{C_3-2C_2}
\det\left[\begin{array}c 1 & 0 & 0 \\ 4 & -3 & 0 \\ 7 & -6 & 0 \end{array}\right]=0
$$

$$
\begin{aligned}
&\det\left[\begin{array}c 2 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 1 & 2
\end{array}\right]\xlongequal[C_3-C_1/2]{C_2-C_1/2}
\det\left[\begin{array}c 2 & 0 & 0 \\ 1 & 1.5 & 0.5 \\ 1 & 0.5 & 1.5 \end{array}\right]\xlongequal{C_3-C_2/3}\\
&\det\left[\begin{array}c 2 & 0 & 0 \\ 1 & 1.5 & 0 \\ 1 & 0.5 & 4/3\end{array}\right]
=2\cdot 1.5\cdot (4/3)\det\left[\begin{array}c 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0.5 & 1/3 & 1\end{array}\right]\xlongequal[C_2-C_3/3]{C_1-C_3/2}\\
&4\det\left[\begin{array}c 1 & 0 & 0 \\ 0.5 & 1 & 0 \\ 0 & 0 & 1\end{array}\right]\xlongequal{C_1-C_2/2}
4\det\left[\begin{array}c 1 & &  \\  & 1 & \\  & & 1\end{array}\right]=4
\end{aligned}
$$

这个过程是对矩阵的列进行高斯消元. 在此过程中如果有全为 $0$ 的列，我们马上得到了行列式为 $0$. (这意味着列并非先行独立，所以核不为 $0$). 如果没有出现全为 $0$ 的列，应该得到一个下三角矩阵. 在此之后我们只需要把对角线上的系数从每一列提出来让对角线全为 $1$. 接下来总是可以很容易地在不改变对角线 $1$ 的情况下消掉对角线下方的所有元素. 所以行列式会等于这些阶梯头，即消元后对角线上元素的乘积.（所以事实上我们并不需要关心把它变成下三角矩阵以后的其它过程，直接把对角线上的元素相乘就可以）

###### 行列式的重要性质

1. 行列式在基变换下保持不变（从某种意义上这是最重要的性质，因为我们最开始的目标就是找到一个不变量）所以如果 $\alpha$ 和 $\beta$ 都是 $V$ 的基底，而 $\varphi:V\to V$ 在这两个基底下的矩阵表示分别为 $A$ 和 $B$, 则 $\det A=\det B$. 此外，任意一个可逆矩阵都可以把基 $\alpha$ 变为一个基 $\alpha P$. 在基 $\beta=\alpha P$ 下，$\varphi$ 的表示应该为 $\beta^{-1}\varphi \beta=(\alpha P)^{-1}\varphi (\alpha P)=P^{-1}(\alpha^{-1}\varphi\alpha)P=P^{-1}A P$. 这推出对任意一个矩阵 $A$ 和任意可逆矩阵 $P$ 都有 $\det P^{-1}A P=\det A$.

   <img src="5-5.svg"/>

2. 多线性、反交换性和 $\det I_n=1$. 这些事我们直接从 $n$-向量（或称之为 $n$ 维有向体积）推导出的性质.

3. 在列加减下保持不变.

4. 如果 $A$ 的列的某个线性组合为 $0$, 则 $\det A=0$. 换言之，此时 $\dim \operatorname{Ker}A>0$, 且 $\rank A<n$.

5. 4的逆否命题：如果 $\det A\ne 0$, 则矩阵的列线性无关且 $\dim\operatorname{Ker}A=0$. 这意味着每个等式 $y=A(x)$, 其中 $y\in\mathbb{R}^n$ 总是有唯一解，此时 $A$ 是双射，存在逆矩阵.

6. 4的逆命题：如果 $\det A=0$, 则 $A$ 中存在线性相关的行. 如果 $\det A=0$, 则它不可能被消元到一个对角线不全为 $0$ 的下三角矩阵，再此过程中，完全是 $0$ 的列会出现，所以 $y=A(x)$ 对有些 $y$ 无解，而对有些有无穷多解.

7. $\det(\varphi\circ\varphi)=\det(\varphi)\det(\varphi)$. 这可以通过映射符合的缩放因子等于两次缩放因子的乘积来解释. 因为缩放因子与所选的基底无关，若 $\det\psi\ne 0$, 则 $\psi(\alpha)$ 也是一个基底，所以
   $$
   \begin{aligned}
   &\det(\varphi\circ\psi)\\
   =&\frac{(\varphi\circ\psi)(\alpha_1)\wedge (\varphi\circ\psi)(\alpha_2)\wedge\cdots\wedge(\varphi\circ\psi)(\alpha_n)}{\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n}\\
   =&\frac{\varphi(\psi(\alpha_1))\wedge \varphi(\psi(\alpha_2))\wedge\cdots\wedge\varphi(\psi(\alpha_n))}{\psi(\alpha_1)\wedge\psi(\alpha_2)\wedge\cdots\wedge\psi(\alpha_n)}
   \frac{\psi(\alpha_1)\wedge\psi(\alpha_2)\wedge\cdots\wedge\psi(\alpha_n)}{\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n}
   \\
   =&\det(\varphi)\det(\psi)
   \end{aligned}
   $$
   如果 $\det\psi\ne 0$, 则 $\operatorname{Ker}(\psi)\ne 0\Rightarrow\operatorname{Ker}(\varphi\circ\psi)\ne 0$, 所以 $\det(\varphi\circ\psi)=0$, 在这种情况下仍然成立. 特别地，取 $V=\mathbb{R}^n$, 则 $\varphi$ 和 $\psi$ 都是 $n\times n$ 矩阵，且 $\det(AB)=\det(A)\det(B)$ 对任意 $A,B\in\mathbb{R}^{n\times n}$ 成立

8. 排列矩阵的行列式是 $\pm 1$. 在一个排列矩阵中，每行每列都仅包含一个 $1$. 所以我们可以通过交换它的行来重新得到 $I_n$. 例如：
   $$
   \begin{aligned}
   &\det\left[\begin{array}c
   & 1 \\ & & & 1 \\ 1 \\ && 1
   \end{array}\right]\xlongequal{C_{12}}
   -\det\left[\begin{array}c
   1 \\ & & & 1 \\ &1 \\ && 1
   \end{array}\right]\xlongequal{C_{24}}\\
   &\det\left[\begin{array}c
   1 \\ & 1 \\ & & &1 \\ && 1
   \end{array}\right]\xlongequal{C_{34}}-\det I_n=-1
   \end{aligned}
   $$
   在每次交换过程中，我们把第 $i$ 列和第 $i$ 行的 $1$ 所在的列交换，最终把行列式转变成 $\det I_n=1$, 最多相差一个符号.

9. $\det A=\det A^{\mathrm{T}}$

   > 请确保你还记得 PLU 分解，并且确认自己明白 $P^{\mathrm{T}}P=I_n$. 如果你忘记了这一点又或者你不关心它是怎么证的，你可以暂时把 $\det A=\det A^{\mathrm{T}}$ 当作一个既定的结果接受下来并且直接向下读. 这个性质告诉我们可以像做列变换一样做行变换. 也就是说你在不使用列变换的前提下得到一个矩阵的行列式. 此外，教材中通常更喜欢做行变换，好处是它让记号更统一，操作更为一致.

   证明这一点需要一些技巧. 首先先对下三角和上三角矩阵证明这个事实：

   对于一个下三角矩阵，它的特征值就是对角线上所有元素的乘积. 而对上三角矩阵，也是这样，我们总是可以消除对角线上方的所有元素.
   $$
   \det\left[\begin{array}c 1 \\ 1 & 4 \\ 5 & 1 &4
   \end{array}\right]\xlongequal[(2)~C_1-C_2/4-5C_3/4]{(1)~C_2-C_3/4}
   \det\left[\begin{array}c 1 \\ & 4 \\ &&4
   \end{array}\right]=1\cdot 4\cdot 4=16\\
   
   \det\left[\begin{array}c 1 & 1 & 5 \\ & 4 & 1 \\ &&4
   \end{array}\right]\xlongequal[(2)~C_3-5C_1-C_3/4]{(1)~C_2-C_1}
   \det\left[\begin{array}c 1 \\ & 4 \\ &&4
   \end{array}\right]=1\cdot 4\cdot 4=16\\
   $$
   (如果 $0$ 出现在对角线上，那么这个三角矩阵的秩就不是 $n$, 从而特征值为 $0$)

   所以对下三角和上三角矩阵，可以写出
   $$
   \det L=\det L^{\mathrm{T}},\det U=\det U^{\mathrm{T}}
   $$
   对于任意 $n\times n$ 的矩阵，PLU 分解把它分解为
   $$
   A=PLU\Rightarrow \det A=\det P\det L\det U
   $$
   由于
   $$
   A^\mathrm{T}=U^{\mathrm{T}}L^{\mathrm{T}}P^{\mathrm{T}}\Rightarrow \det A^{\mathrm{T}}=\det U^{\mathrm{T}}\det L^{\mathrm{T}}\det P^{\mathrm{T}}
   $$
   且其中 $\det L=\det L^{\mathrm{T}}, \det U=\det U^{\mathrm{T}}$. 所以我们只需要证明 $\det P=\det P^{\mathrm{T}}$.

   由于 $P^{\mathrm{T}}P=I_n$, 这意味着 $\det P^{\mathrm{T}}\det P=\det I_n=1$. 当 $\det P=1$ 时有  $\det P^{\mathrm{T}}=1$. 当 $\det P=-1$ 时有 $\det P^{\mathrm{T}}=-1$. 这表明 $\det P=\det P^{\mathrm{T}}$, 完成了证明.

一会我们还会见到行列式的更多性质.

##### 第五步 —— 行列式的第二定义

> 请不要被这一节吓到，它的目的是解释某些教材上吓人的公式. 它基本没有提供关于行列式的直觉，而是把学生淹没在计算的海洋当中.

有些教科书甚至完全不关心行列式的不变性以及一般线性变换的行列式. 它们把一个 $n\times n$ 矩阵的行列式定义为
$$
\det\left[\begin{array}c
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]=\sum_{j_1\sim j_n}
(-1)^{\tau(j_1,j_2,\cdots,j_n)}a_{1j_1}a_{2j_2}\cdots a_{nj_2}
$$
其中 $\tau(j_1, j_2, \cdots, j_n)=\tau(j_1)+\tau(j_2)+\cdots +\tau(j_n)$, 而 $\tau(j_i)$ 对 $i_j$ 之后但是比 $i_j$ 小的数计数. 你可能正在想：这什么鬼啊？这个 $\tau$ 有意义吗？为什么我们要把这些项加起来还要用一个巨大的跑遍所有 $j$ 的求和？它为什么加起来等于 $\det A$. 又或者更确切地说这一大堆东西它真的有意义吗？为了回答这些问题，让我们回到
$$
\det(\varphi)=\frac{\varphi(\alpha_1)\wedge \varphi(\alpha_2)\wedge\cdots\wedge\varphi(\alpha_n)}{\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n}
$$
如果 $A$ 的第 $(i,j)$ 个元素是 $a_{ij}$, 且 $\varphi$ 在基 $\alpha$ 下的表示是 $A$. 则 $\varphi(\alpha)=\alpha A$, 且
$$
\varphi(\alpha_1)=\sum_{i=1}^n a_{i1}\alpha_i,\varphi(\alpha_2)=\sum_{i=1}^n a_{i2}\alpha_i,\cdots,\varphi(\alpha_n)=\sum_{i=1}^n a_{in}\alpha_i
$$
所以
$$
\det(\varphi)=\frac{(\sum_{i=1}^n a_{i1}\alpha_i)\wedge (\sum_{i=1}^n a_{i2}\alpha_i)\wedge\cdots\wedge(\sum_{i=1}^n a_{in}\alpha_i)}{\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n}
$$
接下来把这个公式打开并且把所有的求和提出到括号之外（为了方便，省去了 $1\sim n$ 的范围）从而得到
$$
\det(\varphi)=\sum_{i_1\sim i_n}\frac{a_{i_11}a_{i_22}\cdots a_{i_nn}\alpha_{i_1}\wedge\alpha_{i_2}\wedge\cdots\wedge \alpha_{i_n}}{\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_{n}}
$$
如果 $i_p=i_q$, 则 $\alpha_{i_p}$ 和 $\alpha_{i_q}$ 同时出现在 $\alpha_{i_1}\wedge\alpha_{i_2}\wedge\cdots\wedge\alpha_{i_n}$ 中，所以这些项是 $0$ (因为它们并非线性无关). 只有当 $i_j$ 正好取遍 $1$ 到 $n$ 中的每个值一次的时候才能得到非零值. 所以 $\alpha_{i_1}\wedge \alpha_{i_2}\wedge\cdots\wedge\alpha_{i_n}$ 是 $\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n$ 的一个重排列. 其值为 $\pm \alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n$. 符号由其定向决定. 不妨设 $\alpha_{i_1}\wedge \alpha_{i_2}\wedge\cdots\wedge\alpha_{i_n}$ 的定向为一个函数 $\varepsilon(i_1, i_2, \cdots, i_n)=\dfrac{\alpha_{i_1}\wedge \alpha_{i_2}\wedge\cdots\wedge\alpha_{i_n}}{\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n}$. 则 $\varepsilon$ 只能取 $1$ 和 $-1$, 我们将会在下文中讨论这个符号. 所以我们可以简化等式，并且把它写成
$$
\det A=\det(\varphi)=\sum_{i_1\sim i_n}\varepsilon(i_1,i_2,\cdots, i_n)a_{i_11}a_{i_22}\cdots a_{i_nn}
$$
因为 $\det A=\det A^{\mathrm{T}}$, 我们可以交换 $i$ 和 $j$ 并且把它等价地变成 
$$
\det A=\det A^{\mathrm{T}}=\sum_{j_1\sim j_n}\varepsilon(j_1,j_2,\cdots, j_n)a_{1j_1}a_{2j_2}\cdots a_{nj_n}
$$
现在终于该解决 $\varepsilon(j_1, j_2, \cdots, j_n)$ 的符号了. 为了计算它，首先看一些例子并且找规律. 最简单的非平凡的例子是，当 $n=2$ 时，
$$
\varepsilon(2,1)=\frac{\alpha_2\wedge\alpha_1}{\alpha_1\wedge \alpha_2}=\frac{-\alpha_1\wedge\alpha_2}{\alpha_1\wedge\alpha_2}=-1
$$
接下来，可以不用 $\alpha$ 来写：
$$
\varepsilon(2,1)=-\varepsilon(1,2)=-1
$$
可以用 $2\times 2$ 行列式来检验这个结果：
$$
\det\left[\begin{array}c a_{11} & a_{12} \\ a_{21} & a_{22} \end{array}\right]
=\underset{i_1=1,i_2=2}{\underbrace{\varepsilon(1,2)a_{11}a_{22}}}+
\underset{i_1=2,i_2=1}{\underbrace{\varepsilon(2,1)a_{12}a_{21}}}
=a_{11}a_{22}-a_{12}a_{21}
$$
如果记 $a_{11}=a, a_{12}=b, a_{21}=c, a_{22}=d$, 它就是我们熟悉的 $ad-bc$.

这里 $\varepsilon(1,2)$ 对应到 $\alpha_1\wedge \alpha_2$, 我们称其为标准排列. 而交换两个数字对应交换 $\alpha$ 中的两个向量，所以改变定向.

接下来计算对 $n=3$ 所有排列的结果：
$$
\begin{aligned}
\varepsilon(1,2,3)&=1\\
\varepsilon(1,3,2)&=-\varepsilon(1,2,3)=-1\\
\varepsilon(2,1,3)&=-\varepsilon(1,2,3)=-1\\
\varepsilon(2,3,1)&=-\varepsilon(1,3,2)=\varepsilon(1,2,3)=1\\
\varepsilon(3,1,2)&=-\varepsilon(1,3,2)=\varepsilon(1,2,3)=1\\
\varepsilon(3,2,1)&=-\varepsilon(1,2,3)=-1
\end{aligned}
$$
虽然在第一步中将$1$放在第一位，然后在下一步中将$2$放在第二位非常方便，但事实证明如果用这种方法，在看到一个排列时很难预测会做多少交换（当维数增加时这种现象尤为显著）。为了理解为什么教材上会出现逆序数，让我们从冒泡排序开始——这是一种不高效但对理解和解决这个问题很有帮助的排序算法。

假设我们有一个序列，这个序列是 $1$ 到 $n$ 的一个排列。我们希望重新排列它，使其变得有序（$1, 2,...,n$）。按照以下步骤进行：

从第一个元素开始，将其与第二个元素进行比较，如果它大于第二个元素，则交换它们并将计数加 $1$。继续处理第二个元素，将其与第三个元素进行比较...直到我们到达最后两个元素，然后从第一个元素重新开始。继续这个过程，直到序列变成有序状态。

以数列 $3,1,4,5,2$ 为例：

1. $3>1\Rightarrow {\color{blue}1,3},4,5,2$, $\mathrm{count}=1$.

   $3<4\Rightarrow 1,3,4,5,2$, $\mathrm{count}=1$

   $4<5\Rightarrow 1,3,4,5,2$, $\mathrm{count}=1$

   $5>2\Rightarrow 1,3,4,{\color{blue}2,5}$, $\mathrm{count}=2$

2. $1<3\Rightarrow 1,3,4,2,5$, $\mathrm{count}=2$

   $3<4\Rightarrow 1,3,4,2,5$, $\mathrm{count}=2$

   $4>2\Rightarrow 1,3,{\color{blue}2,4},5$, $\mathrm{count}=3$

3. $1<3\Rightarrow 1,3,2,4,5$, $\mathrm{count}=3$

   $3>2\Rightarrow 1,{\color{blue}2,3},4,5$, $\mathrm{count}=4$

共交换了四次，所以$\varepsilon(3,1,4,2,5)=(-1)^4=1$

对应的python代码是:

```python
def bubble_sort(arr: list[int]) -> int:
    "返回交换的次数"
    n: int = len(arr)
    count: int = 0
    for i in range(n):
        # 为了优化算法的标志，无交换时退出
        swapped: bool = False
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
                count += 1
                swapped = True
        if not swapped:
            break
    return count

# 示例
index_list = [3, 1, 4, 2, 5]
exchanges = bubble_sort(index_list)
print(exchanges)
```

让我们关注一种数对：逆序对。在一个序列中，如果一个数字放在另一个数字之后，且比前面一个数字小（这两个数字并不一定直接相邻，例如$3,1,4,5,2$中的$4$和$2$），我们称之为逆序对。请不要急着问为什么我们要这样做。让我们接着看这个例子：

在序列$3,1,4,5,2$中，有$4$个逆序对：$(3,1),(3,2),(4,2),(5,2)$。

我们在冒泡排序的过程中进行了$4$次交换：

交换$1$和$3$会去除逆序对$(3,1)$，因为它们变成了$(1,3)$，不再是逆序。

交换$2$和$5$会去除逆序对$(5,2)$，因为它们变成了$(2,5)$，不再是逆序。

类似地，其他$2$次交换分别去除了$(4,2)$和$(3,2)$。

事实上，我们共进行了$4$次交换，每次去除一个逆序对。这并非巧合。注意到我们每次只交换相邻的数字。这只是交换了它们的相对位置，但它们相对于其他数字的位置并没有改变。因此，一次交换对应一个逆序对，通过计数逆序对，我们就能知道进行了多少次交换。

最终，记 $\tau(j_i)$ 为出现在 $j_i$ 之后但是比 $j_i$ 小的数的计数（严格意义上应该写作 $\tau_{j_1,j_2,\cdots,j_n}(j_i)$, 因为结果不但依赖于 $j_i$ 自身，还和所有的 $j_1\sim j_n$ 有关）. 所以 $\tau(j_i)$ 计数了所有前一个元素是 $j_i$ 的逆序对. 把它对 $1\sim n$ 求和，总的逆序数应该为 $\tau(j_1,j_2,\cdots,j_n)=\tau(j_1)+\tau(j_2)+\cdots+\tau(j_n)$. 由于逆序数计数了通过冒泡排序将数列排序所需的交换数，$\tau(j_1, j_2, \cdots, j_n)$ 的奇偶性决定了 $\varepsilon(j_1, j_2, \cdots, j_n)$, 其中 $\varepsilon=(-1)^{\tau}$. 最终我们可以清楚地得到结果：
$$
\det\left[\begin{array}c
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]=\sum_{j_1\sim j_n}
(-1)^{\tau(j_1,j_2,\cdots,j_n)}a_{1j_1}a_{2j_2}\cdots a_{nj_2}
$$
这个公式是通过把 $n$-向量的所有分量展开得到的，因为这个定义并不依赖于线性变换，并且定义行列式的方式更加"明确"，所以包括我们教科书在内的一些人更喜欢用这个定义，因为它们认为这更直接. 然而，这个式子的形式意义大于实际意义，此外使用它计算大于 $3\times 3$ 的矩阵行列式非常困难. 就我个人来讲，我更喜欢把这个式子看作 $n$ 维有向体积的和，这种几何直观对我理解并记住它起到了很大帮助.

#### 代数余子式

> 这一小节只是为了解释我们教材中的代数余子式，然而它对理解线性映射的本质并无太大影响，所以如果你在此过程中感到枯燥，不妨跳过.

我们知道一个矩阵的行列式对每列线性，所以如果只让一列变化，同时固定其它 $n-1$ 列，就得到了一个线性映射，把这一列线性映射到行列式，即一个数，所以它是 $\mathbb{R}^n\to\mathbb{R}$ 的.  假设矩阵的第 $2\sim n$ 列都确定了，那么只有 $a_{i1}$ 这些项可以改变，映射具有形式
$$
\det\left[\begin{array}c
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]=c_1a_{11}+c_2a_{21}+\cdots+c_n a_{n1}
$$
这里的 $c_i$ 是与 $A$ 的第一列无关的常数，但是它们会随着其它列的改变而改变. 当把其它列都定下来之后它们就成为了常数.

接下来让我们为 $c_i$ 找到显式的表达式
$$
\begin{aligned}
\det\left[\begin{array}c
a_{11} & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]=&\det\left[\begin{array}c
a_{11} & a_{12} & \cdots & a_{1n}\\
 & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
 & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]+\det\left[\begin{array}c
 & a_{12} & \cdots & a_{1n}\\
a_{21} & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
 & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]\\
&+\cdots+\det\left[\begin{array}c
 & a_{12} & \cdots & a_{1n}\\
 & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]\\
=& a_{11}\det\left[\begin{array}c
1 & a_{12} & \cdots & a_{1n}\\
 & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
 & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]+a_{21}\det\left[\begin{array}c
 & a_{12} & \cdots & a_{1n}\\
1 & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
 & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]\\
&+\cdots +\det\left[\begin{array}c
 & a_{12} & \cdots & a_{1n}\\
 & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
1 & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]
\end{aligned}
$$
所以
$$
c_1=\det\left[\begin{array}c
1 & a_{12} & \cdots & a_{1n}\\
 & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
 & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]=\det\left[\begin{array}c
1 \\
 & a_{22} & \cdots & a_{2n}\\
 & \vdots & \ddots & \vdots\\
 & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]
$$
显然，右侧的行列式等于
$$
\det\left[\begin{array}c
a_{22} & a_{23} & \cdots & a_{2n}\\
a_{31} & a_{32} & \cdots & a_{3n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n2} & a_{n3} & \cdots & a_{nn}\\
\end{array}\right]
$$
(这时因为对前面的行列式高斯消元不改变第一行第一列，所以行列式等于右下角的更小的块的行列式乘以对角线上的 $1$，它们是一样的)

然而为了计算 $c_2$, 我们有必要把第二行换到第一行并且得到
$$
c_2=-\det\left[\begin{array}c
1 \\
 & a_{12} & a_{13} & \cdots & a_{1n}\\
 & a_{32} & a_{33} & \cdots & a_{3n}\\
 & \vdots & \vdots & \ddots & \vdots\\
 & a_{n2} & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]=-\det\left[\begin{array}c
 a_{12} & a_{13} & \cdots & a_{1n}\\
 a_{32} & a_{33} & \cdots & a_{3n}\\
 \vdots & \vdots & \ddots & \vdots\\
 a_{n2} & a_{n2} & \cdots & a_{nn}\\
\end{array}\right]
$$
为了计算 $c_3$, 首先交换第二和第三行，然后交换第一和第二行. 这样让 $a_3$ 浮到了顶端，而同时没有改变其它行之间的相对位置. 这里进行了两次交换，所以前面的符号又变为了正的.

不难发现这样的规律
$$
c_i=(-1)^{i-1}\det\left[\begin{array}c
\vdots & \vdots & \vdots & \vdots\\
a_{i-1,2} & a_{i-1,3} & \cdots & a_{i-1,n}\\
a_{i+1,2} & a_{i+1,3} & \cdots & a_{i+1,n}\\
\vdots & \vdots & \vdots & \vdots
\end{array}\right]
$$
这里的 $(-1)^{i-1}$ 意味着进行了 $i-1$ 次的行交换，矩阵原本的第 $i$ 行和第一列被删除了.

当我们想让第 $j$ 列变化而固定其它的列时，我们发现对应的因子和第一列的类似
$$
c_{ij}=(-1)^{i+j-2}\det\left[\begin{array}c \ddots & \vdots & \vdots & {{\raise{-3pt}\cdot}\kern{1pt}{\raise{1pt}\cdot}\kern{1pt}{\raise{5pt}\cdot}}\\
\cdots & a_{i-1,j-1} & a_{i-1,j+1} & \cdots\\
\cdots & a_{i+1,j-1} & a_{i+1,j+1} & \cdots\\
{{\raise{-2pt}\cdot}\kern{1pt}{\raise{2pt}\cdot}\kern{1pt}{\raise{6pt}\cdot}} & \vdots & \vdots & \ddots\end{array}\right]
$$
$(-1)^{i+j-2}$ 可以被写为 $(-1)^{i+j}$. 但是此处我写成 $(-1)^{i+j-2}$ 的原因是为了把第 $(i,j)$ 个元素换到左上角而不改变其它行和列之间的相对位置，需要经过 $(-1)^{i+j-2}$ 次交换. 在现在的矩阵中，第 $i$ 行和第 $j$ 列被删除了. 这个系数 $c_{ij}$ 被称作 $a_{ij}$ 的代数余子式，对每个 $1\le j\le n$, 有
$$
\det A=\sum_{i=1}^n a_{ij}c_{ij}
$$
这是因为固定下除了第 $j$ 列之外的其它列，行列式变成了第 $j$ 列的线性函数，其系数分别为 $c_{ij}$. 我们称其为行列式的列展开.

类似的讨论可以导出行列式的行展开，对每个 $1\le i\le n$:
$$
\det A=\sum_{j=1}^n a_{ij}c_{ij}
$$

#### 线性方程 —— 行列式的另一个角度

##### Cramer 法则

先回忆一个矩阵 $A:\mathbb{R}^n\to\mathbb{R}^m$ 的行为. 当 $m>n$ 时，$A$ 不可能是满射，总是有一些 $y$ 让 $y=Ax$ 无解. 而对 $n>m$, $A$ 不可能是单射，每个值域中的 $y$ 都对应到很多的 $x$. 所以 $m=n$ 是最"有趣"的情况，此时 $n$ 个方程对应 $n$ 个未知数.

我们知道对 $A:\mathbb{R}^n\to\mathbb{R}^n$, 如果 $\det A=0$, 那么消元会在主对角线上产生 $0$, 此时就有了自由变量，于是 $A$ 不可逆. 但是对 $\det A\ne 0$, $\dim\operatorname{Ker}A=0$. 线性代数基本定理告诉我们 $\dim\operatorname{Im}A=n-\dim\operatorname{Ker}A=n$. 这意味着像是 $\mathbb{R}^n$. 在本节中，我们将会为这样的线性方程的解写出显式的表达式.

把一个线性方程写成如下形式，其中 $\alpha_1\sim\alpha_n$ 为列向量：
$$
y=x_1\alpha_1+x_2\alpha_2+\cdots+x_n\alpha_n
$$
例如，为了解 $x_1$，我们应该消掉所有其它的像. 神奇的是，我们可以通过计算行列式完成这一过程.
$$
\det(y,\alpha_2,\cdots,\alpha_n)=\det(x_1\alpha_1+x_2\alpha_2+\cdots+x_n\alpha_n,\alpha_2,\cdots,\alpha_n)
$$
注意到列相减不改变行列式的值，所以
$$
\det(y,\alpha_2,\cdots,\alpha_n)=\det(x_1\alpha_1,\alpha_2,\cdots,\alpha_n)=x_1\det(\alpha_1,\alpha_2,\cdots,\alpha_n)
$$
至少从形式上，我们可以从 $y_1$ 直接解出 $x_1$. （类似地对于 $x_i$ 的情况，分子上的 $y$ 替换的是 $\alpha_i$ ）
$$
x_1=\frac{\det(y,\alpha_2,\cdots,\alpha_n)}{\det(\alpha_1,\alpha_2,\cdots,\alpha_n)}
$$
分母恰好就是 $\det A$. 这就是 Cramer 法则，下图展示了 $n=2$ 时 Cramer 法则为什么合理.

<img src="5-6.svg"/>

##### 线性方程的判别式

> 本小节讨论的是我个人关于行列式对于线性方程作用的理解.

大部分人对判别式的第一反应可能是 $\Delta=b^2-4ac$. 但是这只是判别式的一部分，判别式实际上可以对任意多项式 $p(x)$, 甚至可以对多变量多项式定义. 判别式反映的是 $p(x)=0$ 的根的情况. 对于一个二次函数 $ax^2+bx+c=0$, 它"倾向于"拥有两个复根，只有当 $\Delta=b^2-4ac=0$ 时才会出现例外. 而对一个方阵 $A$, 如果你随意地在 $\mathbb{R}$ 中取它的系数，它的行列式几乎总是非 $0$ 的. 所以它几乎总是可逆, 此时 $Ax=0$ 有唯一解 $x=0$. 但是如果 $\det A$ 恰好为 $0$, 则 $Ax=0$ 有无穷多解. 这意味着此时出现了"重根"，所以判别式可以视作是一种类型的判别式，它显示出是否有重根.

##### 伴随矩阵

> 本小节只是为了解释课本中的伴随矩阵概念，它对理解线性映射的本质并无太大影响. 所以如果你感到无聊，可以直接跳过它.

我们知道对一个 $\det A\ne 0$ 的线性方程 $y=Ax$，有
$$
x_1=\frac{\det(y,x_2,\cdots,x_n)}{\det A}
$$
我们可以用代数余子式把这个式子展开：
$$
x_1=(\det A)^{-1}\sum_{i=1}^n y_i c_{i1}=(\det A)^{-1}\left[\begin{array}{c}c_{11} & c_{21} & \cdots & c_{n1}\end{array}\right]y
$$
类似地
$$
x_j=(\det A)^{-1}\sum_{i=1}^n y_i c_{ij}=(\det A)^{-1}\left[\begin{array}{c}c_{1j} & c_{2j} & \cdots & c_{ij}\end{array}\right]y
$$
实际上在这个过程中我们已经用 $y$ 解出了 $x$：
$$
x=\left[\begin{array}{c}x_1 \\ x_2 \\ \vdots \\ x_n \end{array}\right]=(\det A)^{-1}\left[\begin{array}{c}
c_{11} & c_{21} & \cdots & c_{n1}\\
c_{12} & c_{22} & \cdots & c_{n2}\\
\vdots & \vdots & \ddots & \vdots\\
c_{1n} & c_{2n} & \cdots & c_{nn}
\end{array}\right]y
$$
于是我们得到了一个把 $A^{-1}$ 用余子式表示的显式表达式.
$$
A^{-1}=(\det A)^{-1}\left[\begin{array}{c}
c_{11} & c_{21} & \cdots & c_{n1}\\
c_{12} & c_{22} & \cdots & c_{n2}\\
\vdots & \vdots & \ddots & \vdots\\
c_{1n} & c_{2n} & \cdots & c_{nn}
\end{array}\right]
$$
这里有 $(A^{-1})_{ji}=(\det A)^{-1} c_{ij}$ (注意此处下标的顺序). 右侧的矩阵被称作伴随矩阵，记作 $\operatorname{adj} A$ （或 $A^*$）. 对于 $\det A\ne 0$, $A^{-1}=(\det A)^{-1}\operatorname{adj}A$, 这可以推出
$$
A\operatorname{adj}A=(\operatorname{adj}A)A=(\det A)I_n
$$
然而这个矩阵乘以其伴随的性质并不依赖于 $\det A\ne 0$. 由余子式的定义，容易验证 $A\operatorname{adj}A=(\det A)I_n=0$ 对 $\det A=0$ 成立，所以这个式子对所有方阵 $A$ 成立.

#### 特征值和特征向量

> 本章中我们将会在 $\mathbb{C}$ 上讨论，因为一个 $n$ 次的多项式在 $\mathbb{C}$ 中总有 $n$ 个根（计重数）

##### 最初的想法

先回忆，当我们希望比较一个变化前和变化后的向量时我们需要找到一些向量，满足线性变换作用在它们上时表现的像一个伸缩. 这给了我们另外一种看待线性变换的角度并且可以帮助我们找到一个好的基底来表示它. 线性变换像一个伸缩一样意味着对某个非零向量 $v$, 有 $\varphi(v)=\lambda v$. 注意到 $\lambda$ 倍的恒同映射总是把 $v$ 映射到 $(\lambda\operatorname{Id})(v)=\lambda v$. 所以只需要把两个式子相减，就存在某些非零向量满足
$$
(\lambda\operatorname{Id}-\varphi)(v)=0
$$
这样一组 $(\lambda, v)$ 被称作特征值和一个对应的特征向量. 对有限维空间而言，这意味着 $\det(\lambda\operatorname{Id}-\varphi)=0$. 这是关于 $\lambda$ 的多项式方程，我们可以先解出 $\lambda$，然后通过解 $(\lambda\operatorname{Id}-\varphi)(v)=0$ 得到 $v$.

更具体地说，首先选一个基并写出 $\varphi$ 的矩阵表示 $A$. 则 $\lambda \operatorname{Id}-\varphi$ 的矩阵表示为 $\lambda I_n-A$. 它们的行列式相同. 然后只需要解出 $(\lambda I_n-A)x=0$, 就可以找到所有特征向量的坐标，例如，假设
$$
A=\left[\begin{array}{c}
3 & 1\\6 & 4
\end{array}\right]
$$
为了找到 $\lambda$ 和 $x$ 使 $Ax=\lambda x$. 先计算
$$
\det(\lambda I_2-A)=\det\left(\lambda\left[\begin{array}{c}
1 & \\ & 1
\end{array}\right]-\left[\begin{array}{c}
3 & 1\\6 & 4
\end{array}\right]\right)=\det\left(\left[\begin{array}{c}
\lambda-3 & 1\\6 & \lambda-4
\end{array}\right]\right)\\
=(\lambda-3)(\lambda-4)-1\cdot 6=\lambda^2-7\lambda+12-6=(\lambda-1)(\lambda-6)
$$
多项式 $p(\lambda)=(\lambda-1)(\lambda-6)$ 称作特征多项式，$p(\lambda)=0$ 被称作特征方程. 解特征方程可得 $\lambda_1=1,\lambda_2=6$. 对 $\lambda_1=1$, 有 $(\lambda_1 I_2-A)x=0\Rightarrow x=t\left[\begin{array}{c}2\\-1\end{array}\right]$. 取 $u_1=\left[\begin{array}{c}2\\-1\end{array}\right]$ 为一个特征向量. 对 $\lambda_2=6$, $(\lambda_2 I_2-A)x=0\Rightarrow x=t\left[\begin{array}{c}1\\3\end{array}\right]$, 取 $u_2=\left[\begin{array}{c}1\\3\end{array}\right]$ 为另一个特征向量，则对所有的 $x=t u_1$, $Ax=\lambda_1 x$, 而对所有的 $x=tu_2$, $Ax=\lambda_2 x$. 所以我们找到了所有的特征值和特征向量.

对于一个 $n$ 维空间，$\deg p=n$. 代数基本定理告诉我们一个 $n$ 次多项式总可以被因式分解为 $p(\lambda)=(\lambda-\lambda_1)(\lambda-\lambda_2)\cdots(\lambda-\lambda_n)$, 这里每个 $\lambda_i$ 都是一个复数.

##### 没有重根的情况

最一般的情况是多项式没有重根，此时对 $i\ne j$ 有 $\lambda_i\ne\lambda_j$. 这给出我们 $n$ 个特征值，所以可以从 $(\lambda_iI_n-A)x=0$ 中解出 $n$ 个特征向量 $u_1\sim u_n$. 矩阵可以写作如下形式：
$$
A\left[\begin{array}{c}u_1 & u_2 & \cdots & u_n\end{array}\right]=
\left[\begin{array}{c}u_1 & u_2 & \cdots & u_n\end{array}\right]
\left[\begin{array}{c}\lambda_1 \\ & \lambda_2 \\ && \ddots \\\ &&& \lambda_n\end{array}\right]
$$
可以通过归纳法证明 $u_1\sim u_n$ 总是先行独立的，所以 $\left[\begin{array}{c}u_1 & u_2 & \cdots & u_n\end{array}\right]$ 作为过渡矩阵（基变换矩阵）把原先选定的基底 $\alpha$ 变成一个新的基底 $\beta$, 使得基 $\beta$ 下的矩阵表示是一个对角矩阵.
$$
\Lambda=\left[\begin{array}{c}\lambda_1 \\ & \lambda_2 \\ && \ddots \\\ &&& \lambda_n\end{array}\right]
$$
这一过程称作"对角化". 这对没有重根出现的情况总是适用. 注意到行列式不依赖于基变化，所以特征多项式实际上是一个不变量，从中解出的特征值也是不变量，这会把每个基底都变到一个只与线性变换 $\varphi$ 有关的基 $\beta$ （最多相差一个特征向量乘的系数，因为特征向量的倍数也是特征向量）

<img src="5-7.svg"/>

##### 有重根的情况

在讨论之前，我们先定义一个叫特征子空间的概念. 对于向量空间 $V$ 上线性变换 $\varphi$ 的某个特征值 $\lambda$. 称对应的特征子空间为 $V_{\lambda}=\left\{v\in V|\varphi(v)=\lambda v\right\}$. 这确实是一个子空间，因为它可以写成 $V_{\lambda}=\operatorname{Ker}(\lambda\operatorname{Id}-\varphi)$. 它包含了所有对应于特征值 $\lambda$ 的特征向量以及 $0$ 向量.

不幸的是，当重根出现时，不一定总能找到一组特征向量组把它们作为基. 这是因为 $\det(\lambda\operatorname{Id}-\varphi)=0$ 只能确存在特征向量 $v$ 满足 $\varphi(v)=\lambda v$, 也就是说对应的特征子空间维数至少是 $1$ 维的. 但是当重根出现时，不能保证重数一定等于子空间的维数. 例如，矩阵 $A=\left[\begin{array}{c}0 & 1 \\ 0 & 0\end{array}\right]$ 的特征方程是 $\lambda^2=0\Rightarrow \lambda=0$. 然而方程 $Ax=0$ 的解只有 $x=\left[\begin{array}{c} t \\ 0 \end{array}\right]$. 这意味着子空间是 $1$ 维的，然而特征值 $\lambda=0$ 作为根的重数是 $2$. 这意味着没有足够的向量构成一个基，矩阵因此不可对角化.

也有时即使出现了特征值，但是矩阵仍然可以对角化. 例如，所有的非零向量都是恒同映射 $\operatorname{Id}$ 特征值为 $1$ 的特征向量. 所以 $\lambda=1$ 对应的子空间是整个空间. 设空间维数为 $n$. 由于没有其它的特征值，所以对应的特征多项式是 $p(\lambda)=(\lambda-1)^n$. 任意取一个 $V$ 的基底，则有 $n$ 个基向量，这个线性映射可以被对角化为一个对角线上全为 $1$ 而其它位置全为 $0$ 的矩阵（即单位矩阵）

称特征方程中特征值 $\lambda$ 的重数为代数重数，而特征子空间维数 $\dim V_{\lambda}$ 为几何重数. 可以给出估计 $1\le\text{几何重数}\le\text{代数重数}$. 如果没有重根，则所有的代数重数都是 $1$, 由不等式可知必须有每个特征值对应的特征子空间维数都为 $1$. 而当出现重根时，代数重数大于 $1$. 只有当所有的几何重数都等于代数重数时这个线性变换才能对角化.

#### 练习

$5.1$ 注意到行变换和列变换都是矩阵空间上的线性变换，先验证下面 $4$ 个矩阵组成 $\mathbb{R}^{2\times 2}$ 的一个基
$$
e_{11}=\left[\begin{array}c
1 & 0 \\ 0 & 0
\end{array}\right],
e_{12}=\left[\begin{array}c
0 & 1 \\ 0 & 0
\end{array}\right],
e_{21}=\left[\begin{array}c
0 & 0 \\ 1 & 0
\end{array}\right],
e_{22}=\left[\begin{array}c
0 & 0 \\ 0 & 1
\end{array}\right]
$$
如果记 $T:\mathbb{R}^{2\times 2}\to\mathbb{R}^{2\times 2}$ 为将第二行减掉两倍的第一行的变换，它在基 $e_{11},e_{12},e_{21},e_{22}$ 下的矩阵表示是什么？

$5.2$ 先验证所有满足 $a_{n+2}=a_{n}+a_{n+1}$ 的数列构成线性空间，并且证明这两个数列 $s_1:1,0,1,1,2,3,\cdots$ 和 $s_2:0,1,1,2,3,5,\cdots$ 构成线性空间的一个基. 由于去除前 $t$ 项的操作 $R_t$ 是这个空间上的线性映射，写出 $R_1, R_2, R_3$ 的矩阵表示 $A_1, A_2, A_3$. 并且叙述 $A_1, A_2, A_3$ 之间有何联系.

$5.3$ 计算 $\mathbb{R}^2$ 中顶点为 $(0, 0), (3, 1), (9, 5), (6, 4)$ 的平行四边形通常的(即不包含定向的)面积.

$5.4$ 使用 $(u-v)\wedge (u-v)=0$ 证明 $u\wedge v=-v\wedge u$.

$5.5$ 如果 $\left[\begin{array}c\beta_1 & \beta_2 & \beta_3\end{array}\right]=\left[\begin{array}c\alpha_1 & \alpha_2 & \alpha_3\end{array}\right] A$, 也就是说
$$
\beta_1=a_{11}\alpha_1+a_{21}\alpha_2+a_{31}\alpha_3\\
\beta_2=a_{12}\alpha_1+a_{22}\alpha_2+a_{32}\alpha_3\\
\beta_3=a_{13}\alpha_1+a_{23}\alpha_2+a_{33}\alpha_3
$$
把 $\beta_1\wedge \beta_2\wedge \beta_3$ 的表达式展开并把它用 $\alpha_1\wedge\alpha_2\wedge\alpha_3$ 表示. 观察其中有多少非零的项，并把它们的符号和所有三个数字排列的定向比较.

$5.6$ 分别通过消元和你在 $5.5$ 中得到的结果计算如下行列式：
$$
\det\left[\begin{array}c 3 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 3
\end{array}\right]
$$
$5.7$ 由于 $\det A=\det A^{\mathrm{T}}$. 对正交矩阵 $Q$, 证明 $|\det Q|=1$.

$5.8$ 计算 $\varepsilon(3,1,5,2,4)$ 并计算下列矩阵的行列式：
$$
\det\left[\begin{array}c && 3 \\ 1 \\ &&&& 5 \\ & 2 \\ &&& 4
\end{array}\right]
$$
$5.9$ 计算如下的行列式，结果会是关于向量 $x$ 的线性函数，$x$ 的每个分量前的系数分别是什么？如果使用 $\mathbb{R}^3$ 上的标准内积，它可以表示成哪个向量和 $x$ 的内积，结果何时为 $0$？
$$
\det\left[\begin{array}c x_1 & 1 & 4 \\ x_2 & 2 & 5 \\ x_3 & 3 & 6
\end{array}\right]
$$
$5.10$ 分别使用 Cramer 法则和消元解下列方程：
$$
\left[\begin{array}c 3 \\ 1 \\ 4
\end{array}\right]=
\left[\begin{array}c 1 \\ 1 \\ 1
\end{array}\right]x_1 +
\left[\begin{array}c 1 \\ 2 \\ 2
\end{array}\right]x_2 +
\left[\begin{array}c 1 \\ 2 \\ 3
\end{array}\right]x_3
$$
$5.11$ 为 $\mathbb{R}^2$ 找一个基 $U$ 以对角化 $$A=\left[\begin{array}c 1 & 1 \\ & 2
\end{array}\right]$$. 则 $AU=U\Lambda\Rightarrow A=U\Lambda U^{-1}$. 根据 $A=U\Lambda U^{-1}$ 计算 $A^{127}$, 你可以从中体会到对角化为什么让计算矩阵的幂更加方便.

$5.12$ 证明如果一个线性映射 $\varphi$ 可对角化，则 $\det\varphi=\lambda_1\lambda_2\cdots\lambda_n$. （提示：行列式无关于基底选取）

$5.13$ 证明一个方阵不可逆当且仅当它有 $0$ 特征值，为什么这个结果成立.

$5.14$ 如果 $\lambda$ 是 $\varphi$ 的一个特征值，证明 $\lambda^2$ 是 $\varphi^2=\varphi\circ\varphi$ 的一个特征值，且 $v$ 仍然是 $\varphi^2$ 的一个特征向量.

$5.15$ 如果 $p(\lambda)$ 是某个可对角化矩阵 $A$ 的特征多项式，根据 $A=U\Lambda U^{-1}$ 证明 $p(A)=0$. 这里 $p(A)$ 的定义是 $p(A)=p_0 I_n+p_1 A + p_2 A^2 + \cdots + p_{n-1} A^{n-1} + p_n A^n$. 齐总 $p_i$ 是 $\lambda^i$ 项的系数

$5.16$ 如果 $A\in\mathbb{R}^{m\times m}, B\in\mathbb{R}^{n\times n}, C\in\mathbb{R}^{m\times n}$. 证明 $\det\left[\begin{array}c A  & C\\ & B\end{array}\right]=\det A\cdot\det B$. （提示：把问题分成两半：$\det A=0$ 与 $\det A\ne 0$）

$5.17$ $A=\left[\begin{array}c 1  & 1 \\ & 1 & 1 \\ & & 1\end{array}\right]$ 可对角化吗？计算 $A^2, A^3,A^4$ 并猜想 $A^k$ 的结果.

$5.18$ 证明除了恒同映射以外，没有线性映射可以被相似对角化为单位矩阵.