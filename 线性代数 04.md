### 从 $(\mathbb{R}^n, \cdot)$ 到 $H$ —— 内积和正交化

> 本章中我将会始终讨论 $\mathbb{R}$ 上的向量空间，而非像之前的章节那样总可以自然地拓展到任何一个域 $K$ 上. 在某些教科书中，在复空间上定义内积是合理的，它从某些意义上比实数内积更复杂. 但是简洁起见，我们只讨论实向量空间. 字母 $\alpha$ 和 $\beta$ 可能被用来指代向量或者基底，这取决于上下文.

#### 什么是内积？

如果 $H$ 是一个实向量空间，那么一个内积以两个向量作为参数并且得到一个实数，它反映了两个向量之间的相似或者说同方向性，我们高中学过的点乘就是一个重要例子. 所以内积从形式上是 $\langle-,-\rangle: H\times H\to \mathbb{R}$. 在高中，点乘通常被定义为 $\alpha\cdot\beta = |\alpha||\beta|\cos\angle(\alpha,\beta)$, 其中 $\angle(\alpha,\beta)$ 是两个向量之间的夹角. 如果内积是正的，这意味着两个向量处在相似的方向（成锐角），如果内积是负的，这意味着两个向量处在相反的方向（成钝角），如果内积是 $0$, 它们两个向量就是正交的（成直角）. 为了用这种方式定义内积，我们需要两个函数. 第一个是模（或称之为模长、长度、甚至是绝对值）$|-|:H\to \mathbb{R}_{\ge 0}$. 第二个是向量间夹角 $\angle: H\times H\to [0, \pi]$. 这种方法在欧氏空间中走得通，但是来到一般的实向量空间时就开始效果堪忧了，因为这个空间不一定自然拥有模和夹角. 此外这么定义会让一些性质的证明变得困难，比如说：$\alpha\cdot (\beta_1+\beta_2)=\alpha\cdot \beta_1+\alpha\cdot\beta_2$. 这就是为什么我们需要重新定义一个内积以让我们处理问题更加方便.

重新定义一个概念并不是一件容易的事情，我们需要确保它和之前的概念兼容. 之前的路线问题出在：一个一般的空间上并不天然拥有有一个度量模长和角度的函数，而我们最终的解决方案是先定义内积，然后反过来定义模和角度.

就像我们之前定义向量空间的方法一样，我们先通过几条规则来定义什么可以称之为内积. 内积是一个 $\langle -,- \rangle: H\times H\to \mathbb{R}$ 的函数，对任意的 $\alpha,\beta\in H$ 和 $k_1,k_2\in\mathbb{R}$ 满足：

1. $\langle\alpha,\beta\rangle = \langle\beta, \alpha\rangle$ （对称性）

2. $\langle k_1\alpha_1+k_2\alpha_2,\beta\rangle=k_1\langle\alpha_1,\beta\rangle + k_2\langle\alpha_2,\beta\rangle$ （对第一个参数线性）

   注：前两条规则可以推导出 $\langle\alpha, k_1\beta_1+k_2\beta_2\rangle=\langle k_1\beta_1+k_2\beta_2,\alpha\rangle=k_1\langle\beta_1, \alpha\rangle+k_2\langle\beta_2,\alpha\rangle=k_1\langle\alpha, \beta_1\rangle+k_2\langle\alpha,\beta_2\rangle$, 所以事实上它也对第二个变量线性，这个性质被称为双线性

3. 对于 $\alpha\ne 0, \langle \alpha,\alpha\rangle>0$ （正定性）

   注：$\langle0,0\rangle=0\langle 0,0\rangle$ 可以推出 $\langle 0, 0\rangle=0$

为了和公式 $\langle \alpha,\alpha\rangle=|\alpha||\alpha|\cos\angle(\alpha,\alpha)=|\alpha|^2\cos 0=|\alpha|^2$ 兼容，我们需要重新定义模长为 $|\alpha|=\sqrt{\langle\alpha,\alpha\rangle}$, 并且重新定义角度为 $\angle(\alpha,\beta)=\arccos\dfrac{\langle\alpha,\beta\rangle}{|\alpha||\beta|}$. 如果 $\langle \alpha,\beta\rangle=0$, 称 $\alpha$ 和 $\beta$ 正交（垂直），记作 $\alpha\perp \beta$. 这个定义和某些读者的直觉可能不太相符，但是众所周知度量角的大小是为了描述一个物体相对另一个物体的定向，这与内积的目的基本相同. 此外，在计算中，计算内积通常比计算角度更加容易. 几何的基本对象是长度和角度，而二者都可以由内积导出，所以一个向量空间的"几何"完全由其上定义的内积决定. 这里最重要的一点是：内积取代了长度和角度这些，成为了最基本的概念.

#### 转置和 $\mathbb{R}^n$ 中的标准内积

对于两个向量 $x, y\in H= \mathbb{R}^n$, 读者可能已经在高中课上学过它们的点积是 $\displaystyle x\cdot y=\sum_{i=1}^n x_i y_i$. 在我们已经重定义了内积的前提下，为了确认点积确实是一种内积，我们需要验证它满足内积的三个条件. 这留给读者，作为练习. 点积也被称作 $\mathbb{R}^n$ 上的标准内积，但是我们只在讨论 $\mathbb{R}^n$ 时使用"标准内积"这个说法，这是因为 $\mathbb{R}^n$ 有一组标准的基底 $e_1, e_2, \cdots, e_n$, 但是大部分的线性空间并没有.

为了把这个过程看的更清楚，不妨引入转置的概念. 一个矩阵的转置就是把它的列重新写成行，$A$ 的转置被记作 $A^{\mathrm{T}}$, 例如：
$$
\left[\begin{array}{c}
\color{red}1 & \color{blue}4\\
\color{red}2 & \color{blue}5\\
\color{red}3 & \color{blue}6
\end{array}\right]^{\mathrm{T}}=
\left[\begin{array}{c}
\color{red}1 & \color{red}2 & \color{red}3\\
\color{blue}4 & \color{blue}5 & \color{blue}6
\end{array}\right]
$$
所以一个行向量的转置是一个列向量，而一个列向量的转置是一个行向量. 在引入转置的记号后，$\mathbb{R}^n$ 上的标准内积可以被写成 $x\cdot y=x^{\mathrm{T}}y$, 也就是说：
$$
x\cdot y=\left[\begin{array}c
x_1 \\ x_2 \\ \vdots \\ x_n 
\end{array}\right]\cdot
\left[\begin{array}c
y_1 \\ y_2 \\ \vdots \\ y_n 
\end{array}\right]
=\left[\begin{array}c
x_1 & x_2 & \cdots & x_n 
\end{array}\right]
\left[\begin{array}c
y_1 \\ y_2 \\ \vdots \\ y_n 
\end{array}\right]
=\sum_{i=1}^n x_i y_i
$$

> 注：转置的重要性质是 $(AB)^{\mathrm{T}}=B^{\mathrm{T}}A^{\mathrm{T}}$, 最后一节中将会用到这个性质. 这可以通过把每一项展开证明：设 $a_{ik}, b_{kj}, c_{ij}$ 分别为 $A$ 的第 $(i, k)$ 项，$B$ 的 $(k, j)$ 项，$AB$ 的第 $(i, j)$ 项，则 $c'_{ji}=c_{ij}=\sum_k a_{ik}b_{kj}=\sum_k b'_{jk}a'_{ki}$, 在 $(AB)^{\mathrm{T}}$ 中， $c'_{ji}=c_{ij}=\sum_k a_{ik}b_{kj}=\sum_k b'_{jk}a'_{ki}$. 对每个元素都有 $c_{ji}'=\sum_k b_{jk}'a_{ki}'$, 这推出 $(AB)^{\mathrm{T}}=B^{\mathrm{T}}A^{\mathrm{T}}$

### 非标准内积和正定矩阵

既然我们把熟悉的内积叫做"标准"内积，你可能在想，有什么"非标准"的内积吗？事实上，在内积的新定义下，有无穷多个内积，而标准内积只是其中之一. 例如，考虑 $\langle x,y\rangle=2x^{\mathrm{T}}y$, 我们就得到了一个"非标准"的内积. 现在摆在我们面前的有三个问题：

1. 我们真的需要这些"非标准"的内积吗？
2. 我们可以找到所有的内积吗？
3. 这些内积有什么共同点？

以下是我对这些问题的答案

1. 有如下几个原因：

   (1) 加权内积：加权内积会为向量的不同分量加上不同的权重，通常用在数值分析或有限元分析中用于模拟具有不同性质的物理现象. 当某些分量比较重要，而另一个不那么重要的时候，通常使用"加权"的内积，给每个变量分配一个权重，例如，在 $H=\mathbb{R}^2$ 上定义的 $\langle x, y\rangle=2x_1y_1+x_2y_2$ 给第一个分量权重 $2$, 给第二个分量权重 $1$

   (2) 内积中的交叉项：基向量并不总是垂直的，这时可能出现交叉项. 例如，假设我们在讨论空间 $H=\set{x\in\mathbb{R}^3|x_1+x_2=x_3}$, 映射 $\alpha: \mathbb{R}^2\to H, (x_1,x_2)\mapsto (x_1,x_2,x_1+x_2)$ 是一个同构，并且给出了 $H$ 的一个基，其中基向量 $\alpha_1=\alpha(e_1)=(1,0,1)$, $\alpha_2=\alpha(e_2)=(0,1,1)$, 所以 $(x_1,x_2,x_1+x_2)$ 在基 $\alpha$ 下的坐标为 $(x_1,x_2)$. 坐标为 $(x_1,x_2)$ 的向量和坐标为 $(y_1, y_2)$ 的向量的内积是 $(x_1,x_2,x_1+x_2)\cdot (y_1,y_2,y_1+y_2)=2x_1y_1+2x_2y_2+x_1y_2+x_2y_1$. $2$ 表明基向量的长度不是 $1$ (实际上长度是 $\sqrt 2$), 而 $x_1y_2$ 和 $x_2y_1$ 这样的交叉项表明基向量并非垂直（实际上角度为 $\pi/3$）

2. 为了研究 $H\times H$ 上双线性函数的性质，我们只需要知道它如何作用于基向量就可以了. 先说 $\mathbb{R}^n$, $\mathbb{R}^n$ 有一组向量 $e_1, e_2, \cdots, e_n$ 作为基底，所以
   $$
   \begin{aligned}
   \langle x,y\rangle=&\left\langle \sum_{i=1}^n x_ie_i,\sum_{j=1}^n y_je_j \right\rangle\\
   =& \sum_{i=1}^n x_i\left\langle e_i,\sum_{j=1}^n y_je_j \right\rangle\\
   =&\sum_{i=1}^n x_i \sum_{j=1}^n y_j \langle e_i,e_j\rangle\\
   =&\sum_{i=1}^n\sum_{j=1}^n\langle e_i,e_j\rangle x_i y_j
   \end{aligned}
   $$
   记 $g_{ij}=\langle e_i, e_j\rangle$, 则一个内积可以写成
   $$
   \langle x, y\rangle = \sum_{1\le i,j\le n} g_{ij}x_iy_j
   $$
   在展开式子的过程中，第二条性质（双线性）自动满足了，所以我们需要验证其它的两条性质. 第一条性质是对称性，这要求
   $$
   \langle x, y\rangle = \sum_{1\le i,j\le n} g_{ij}x_iy_j=\langle y, x\rangle=\sum_{1\le i,j\le n} g_{ij}y_ix_j=\sum_{1\le i,j\le n} g_{ji}x_iy_j
   $$
   所以对任意的 $1\le i, j\le n$, 有 $g_{ij}=g_{ji}$ 成立

   而最后一条性质：正定性，这要求非零向量和自身的内积是正的，也就是说对 $x\ne 0$：
   $$
   \langle x, x\rangle = \sum_{1\le i,j\le n}g_{ij}x_ix_j> 0
   $$
   而内积也可以用矩阵语言来表示
   $$
   \langle x, y\rangle = x^{\mathrm{T}}Gy,\text{其中~}G=\left[\begin{array}c
   g_{11} & g_{12} & \cdots & g_{1n}\\
   g_{21} & g_{22} & \cdots & g_{2n}\\
   \vdots & \vdots & \ddots & \vdots\\
   g_{n1} & g_{n2} & \cdots & g_{nn}
   \end{array}\right]
   $$
   为了看得更清楚，$G$ 可以被视作接受两个向量输入并且给出一个数字作为输出（如下的公式块只是为了便于理解，并非正式数学语言）
   $$
   x^{\mathrm{T}}Gy\to \left[\begin{array}{c}
   && \color{blue}y_1 & \color{blue}y_2 & \color{blue}\cdots & \color{blue}y_n\\
   && \color{blue}\downarrow & \color{blue}\downarrow & \color{blue}\cdots & \color{blue}\downarrow\\
   \color{red}x_1 & \color{red}\color{red}\rightarrow & 
   g_{11} & g_{12} & \cdots & g_{1n}\\
   \color{red}x_2 & \color{red}\rightarrow & 
   g_{21} & g_{22} & \cdots & g_{2n}\\
   \color{red}\vdots & \color{red}\vdots 
   & \vdots & \vdots & \ddots & \vdots\\
   \color{red}x_n & \color{red}\rightarrow 
   & g_{n1} & g_{n2} & \cdots & g_{nn}
   \end{array}\right]\to
   \sum_{\text{全部}}
   \left[\begin{array}c
   g_{11}{\color{red}x_1}{\color{blue}y_1} &
   g_{12}{\color{red}x_1}{\color{blue}y_2} & \cdots & 
   g_{1n}{\color{red}x_1}{\color{blue}y_n}\\
   g_{21}{\color{red}x_2}{\color{blue}y_1} & 
   g_{22}{\color{red}x_2}{\color{blue}y_2} & \cdots & 
   g_{2n}{\color{red}x_2}{\color{blue}y_n}\\
   \vdots & \vdots & \ddots & \vdots\\
   g_{n1}{\color{red}x_n}{\color{blue}y_1} & 
   g_{n2}{\color{red}x_n}{\color{blue}y_2} & \cdots & 
   g_{nn}{\color{red}x_n}{\color{blue}y_n}
   \end{array}\right]
   $$
   这个矩阵必须对称 ( $G^{\mathrm{T}}=G$, 这是因为 $g_{ij}=g_{ji}$ ), 此外对 $x\ne 0$ 有 $x^{\mathrm{T}}Gx>0$. 满足这一条件的矩阵被称作正定矩阵. 需要注意的是 $f(x)=x^{\mathrm{T}}Gx$ 是关于 $x$ 的齐次二次函数.（不要和线性函数混淆）更多关于正定的内容将会在第6章中呈现，在此我们只需要对它在 $0$ 处取得极小值有印象就可以了. 所以 $\mathbb{R}^n$ 上的内积和正定矩阵一一对应，矩阵 $G$ 通常被称作度量矩阵.（或度规）特别地，令 $G=I_n$, 即 $n\times n$ 单位矩阵，则它很显然对称，且 $x^{\mathrm{T}}Gx=x^{\mathrm{T}}I_n x=x^{\mathrm{T}}x=x_1^2+x_2^2+\cdots +x_n^2\ge 0$, 仅在 $0$ 处取到 $0$ 值，这恰好就是标准内积. 对于一般的空间，只需把这里的基底换成对应的基底即可.

3. 定义内积的时候，我们只依赖于三条性质：对称性、双线性和正定性，所以在证明相关性质时我们只有，实际上也只需要它们. （性质直接从三条规则导出，不依赖于 $\mathbb{R}^n$）在高中时我们已经学过了关于向量，特别是关于内积和长度的很多结果：比如柯西不等式告诉我们 $\alpha\cdot\beta\le |\alpha||\beta|$, 此外还有余弦定理，三角不等式，又或许你学过一个向量在另一个向量的投影. 下文将会证明这些结果可以直接拓展到任意的内积空间.

   (1) 柯西不等式

   你是否想过，在没有验证 $|\langle \alpha,\beta\rangle|\le |\alpha||\beta|$ 的情况下，前文为什么能能够贸然取 $\arccos$ 呢？接下来我将会使用一点点技巧来证明它确实是对的.

   考虑如下函数
   $$
   f(t)=\langle \alpha+t\beta,\alpha+t\beta\rangle = \langle\alpha,\alpha\rangle+2t\langle\alpha,\beta\rangle+t^2\langle\beta,\beta\rangle
   $$
   由于夹角只对非零向量定义，这里有 $\langle \beta,\beta\rangle\ne 0$, 所以它是二次函数. 正定推出 $f(t)\ge 0$, 当且仅当 $\alpha+t\beta=0$ 时取等号. 由于这个函数最多有一个实零点，其 $\Delta=b^2-4ac=4\langle\alpha,\beta\rangle^2-4\langle\alpha,\alpha\rangle\langle\beta,\beta\rangle\le 0$, 仅在 $\alpha+t\beta=0$ 对某个 $t$ 成立时取等号，$\langle\alpha,\beta\rangle^2\le \langle\alpha,\alpha\rangle\langle\beta,\beta\rangle=|\alpha|^2|\beta|^2\Rightarrow |\langle\alpha,\beta\rangle|\le|\alpha||\beta|$ 等号成立当且仅当 $\alpha$ 和 $\beta$ 共线

   (2) 余弦定理
   $$
   |\alpha-\beta|=\sqrt{\langle\alpha-\beta,\alpha-\beta \rangle}=\sqrt{\langle\alpha,\alpha\rangle + \langle\beta,\beta\rangle - 2\langle\alpha,\beta\rangle}=\sqrt{|\alpha|^2+|\beta|^2-2|\alpha||\beta|\cos\theta}
   $$
   用我们更为熟悉的语言，如果在三角形 $OAB$ 中，$\alpha=\vec{OA},\beta=\vec{OB}$, 则

   $|\vec{AB}|=|\alpha-\beta|=\sqrt{|\vec{OA}|^2+|\vec{OB}|^2-2|\vec{OA}||\vec{OB}|\cos\theta}$

   特别地，当 $\alpha\perp\beta$ 时，$\langle\alpha,\beta\rangle$ 这一项变为了 $0$, 得到了勾股定理

   (3) 三角不等式

   $|\alpha+\beta|=\sqrt{|\alpha|^2+|\beta|^2+2|\alpha||\beta|\cos\theta}\le\sqrt{|\alpha|^2+|\beta|^2+2|\alpha||\beta|}=\sqrt{(|\alpha|+|\beta|)^2}=|\alpha|+|\beta|$

   (4) 一个向量在其它向量上的投影

   对 $\alpha,\beta\in H$, 向量 $\beta$ 在 $\alpha$ 上的投影为 $\dfrac{\langle\alpha,\beta\rangle}{\langle\alpha,\alpha\rangle}\alpha$. 这是因为
   $$
   \frac{\langle\alpha,\beta\rangle}{\langle\alpha,\alpha\rangle}\alpha
   =\frac{|\alpha||\beta|\cos\angle(\alpha,\beta)}{|\alpha|^2}|\alpha|\frac{\alpha}{|\alpha|}=(|\beta|\cos\angle(\alpha,\beta))\frac{\alpha}{|\alpha|}
   $$
   $|\beta|\cos\angle(\alpha,\beta)$ 是投影长度，而 $\dfrac{\alpha}{|\alpha|}$ 是单位方向向量，为了方便，记 $\beta$ 在 $\alpha$ 上的投影为 $\operatorname{proj}_{\alpha}(\beta)$.

#### 等距同态和施密特正交化

现在我要讲讲我们的标题了，"从 $(\mathbb{R}^n, \cdot)$ 到 $H$" 也可以指一种映射. 众所周知一个线性映射保持了加法和标量乘法，而现在应该内积空间在向量空间的基础上多了一个内积，这意味着 $H$ 可以看成一个 $4$ 元组
$$
(H, \text{向量加法}, \text{标量乘法}, \text{内积})
$$
$(\mathbb{R}^n, \cdot)$ 是一个特例，以其上的点积作为内积：
$$
(\mathbb{R}^n, \text{向量加法}, \text{标量乘法}, \text{点积})
$$
如果一个映射保持了"结构"，除了要是线性的，他还应该保持内积. 这样的一个映射被称作等距同态，这是由"等距"和"同态"构成的复合词，反映了它不仅是线性映射(同态)，还保持度量.

初等线性代数和表示论的一个主要目标就是把(有限维)抽象空间中的向量用它们在某个基底下的坐标来坐标，即 $\mathbb{R}^n$ 中的向量表示，并且通过更为具体的矩阵来研究抽象的映射. 对于一个有限维向量空间 $H$ 的一个基底 $\alpha$, $H$ 上的度量自然诱导出一个 $\mathbb{R}^n$ 上的度量 ( $g_{ij}=\langle e_i, e_j\rangle$ 由 $\langle \alpha(e_i),\alpha(e_j)\rangle$ 决定 ). 既然 $\mathbb{R}^n$ 上有一个标准内积，一个好想法是找到一个基底，使空间的度量诱导出的是 $\mathbb{R}^n$ 上的标准内积. 换言之，对 $i=j$, $g_{ij}=1$, 对 $i\ne j$, $g_{ij}=0$. 所以 $\langle \alpha_i, \alpha_i\rangle =1$, 这表明 $|\alpha_i|=1$, 而对 $i\ne j$, $\langle\alpha_i, \alpha_j\rangle=0$, 这表明 $\alpha_i\perp\alpha_j$. 这样的一个基 $\left[\begin{array}c\alpha_1 & \alpha_2 & \cdots & \alpha_n\end{array}\right]$ 被称为一个标准正交基（或归一正交基）被称作"标准"是因为它诱导出的 $\mathbb{R}^n$ 上的内积是标准内积.

回想我们先前如何获得有限维向量空间的一个基底：先找到一组足以"表达"整个空间的向量，然后逐渐去除线性相关的直到获得一组线性无关的向量. 为了从一个基底得到一个正交基，我们要做的大体上差不多. 不过这次我们要去除的不是多余的向量，而是向量中不垂直的多余的"部分". 如下的过程称作施密特正交化：

从 $H$ 的一个基底 $\alpha=\left[\begin{array}c\alpha_1 & \alpha_2 & \cdots & \alpha_n \end{array}\right]$ 开始

1. 令 $\beta_1=\alpha_1$

   令 $\beta_2=\alpha_2-\operatorname{proj}_{\beta_1}(\alpha_2)$, 则 $\beta_2\perp \beta_1$

   令 $\beta_3=\alpha_3-\operatorname{proj}_{\beta_1}(\alpha_3)-\operatorname{proj}_{\beta_2}(\alpha_3)$, 则 $\beta_3\perp \beta_1$ 且 $\beta_3\perp\beta_2$.

   ......

   令 $\beta_n=\alpha_n-\operatorname{proj}_{\beta_1}(\alpha_n)-\operatorname{proj}_{\beta_2}(\alpha_n)-\cdots -\operatorname{proj}_{\beta_{n-1}}(\alpha_n) $, 则 $\beta_n\perp\beta_i$ 对 $i<n$ 成立

2. 最终把每个向量除以它的模，以让它们模长为 $1$，即归一化
   $$
   \varepsilon_i=\frac{\beta_i}{|\beta_i|}
   $$
   $\langle \varepsilon_i, \varepsilon_j\rangle=\left\{\begin{array}c 1, & i=j\\ 0, & i\ne j \end{array}\right.$   是 $H$ 的一个归一正交基


在 python 代码中，这个过程写作（设 $\alpha_i$ 均为 $\mathbb{R}^n$ 的向量）

```python
# numpy 提供了方便的数组运算
import numpy as np
# ndarray 在 numpy 中是 n 维数组类型
def Gram_Schmidt(a: list[np.ndarray]) -> list[np.ndarray]:
    # 获得向量数量
    n: int = len(a)
    # 从 alpha_1 开始
    b: list[np.ndarray] = [a[0]]
    for j: int in range(1, n):
        v: np.ndarray = a[j]
        for i: int in range(i):
            bi_dot_aj: float = b[i].dot(a[j])
            bi_dot_bi: float = b[i].dot(b[i])
            v -= bi_dot_aj / bi_dot_bi * b[i]
        # 设置 b[j] 为 v
        b.append(v)
    # np.linalg.norm 计算 |beta_i|
    return [bi / np.linalg.norm(bi) for bi in b]
```

你是否认为这个操作看起来很熟悉？在第2章中高斯消元引出了矩阵的LU(或者PLU)分解. 因为我们在高斯消元中做行操作，我们从左侧分离出了一个下三角矩阵，它编码了我们操作矩阵的过程. 而在正交化过程中，我们在对向量操作，这些向量作为矩阵的列，应该从右侧分离出一个记录了我们如何操作向量组的矩阵.

同样地，首先把这个过程改写成矩阵形式（这些向量作为列，但本身不必是列向量）用 $\gamma_1$ 表示向量之间的相减，$\gamma_2$ 表示除以模长的过程:
$$
\left[\begin{array}c \beta_1 & \alpha_2 & \cdots & \alpha_n\end{array}\right]
\xrightarrow{\gamma_{12}} 
\left[\begin{array}c \beta_1 & \beta_2 & \cdots & \alpha_n\end{array}\right]\rightarrow\cdots \\ \xrightarrow{\gamma_{1n}}
\
\left[\begin{array}c \beta_1 & \beta_2 & \cdots & \beta_n\end{array}\right]
\xrightarrow{\gamma_{2}}
\left[\begin{array}c \varepsilon_1 & \varepsilon_2 & \cdots & \varepsilon_n\end{array}\right]
$$
对于每个 $j\ge 2$, 在 $\gamma_{1j}$ 中，$\alpha_j$ 被减去每个满足 $1\le i<j$ 的 $\beta_j$ 的倍数，记这个系数为 $c_{ij}=\dfrac{\langle \beta_i, \alpha_j\rangle}{\langle \beta_i, \beta_i\rangle}$, 则这些列操作可以为
$$
\alpha\xrightarrow{\gamma_{12}:~C_2-c_{12}C_1}\xrightarrow{\gamma_{13}:~C_3-c_{13}C_1-c_{23}C_2}\cdots
$$
现在来关注 $\gamma_{1j}$:

对 $i<j$ ( 右侧的矢量只有 $\color{blue}1$ 个非零的数字 )
$$
{\color{blue}\beta_i}=\left[\begin{array}c \color{blue}\beta_1 & \color{blue}\cdots & \color{blue}\beta_i  & {\color{blue}\cdot}\,{\color{red}\cdot}\,{\color{red}\cdot} & \color{red}\alpha_n \end{array}\right]
\left[\begin{array}c \color{lightblue}0 \\ \color{lightblue}\vdots \\ \color{blue}1 \\ 
{\color{white}\vdots}%\kern -2.78pt
% \raise{5.92pt}{\color{lightblue}\cdot}\kern -2.78pt
% \raise{1.87pt}{\color{pink}\cdot}\kern -2.78pt
% \raise{-2.19pt}{\color{pink}\cdot} 
\\
\color{pink}0 \end{array}\right]
$$
对于 $i>j$ ( 右侧的矢量只有 $\color{red}1$ 个非零的数字 )
$$
{\color{red}\alpha_i}=\left[\begin{array}c \color{blue}\beta_1 & {\color{blue}\cdot}\,{\color{blue}\cdot}\,{\color{red}\cdot} & \color{red}\alpha_i  & {\color{red}\cdots} & \color{red}\alpha_n \end{array}\right]
\left[\begin{array}c \color{lightblue}0 \\
{\color{white}\vdots}%\kern -2.78pt
% \raise{5.92pt}{\color{lightblue}\cdot}\kern -2.78pt
% \raise{1.87pt}{\color{lightblue}\cdot}\kern -2.78pt
% \raise{-2.19pt}{\color{pink}\cdot} 
\\
\color{red}1 \\ 
\color{pink}\vdots \\ \color{pink}0 \end{array}\right]
$$
对于第 $j$ 列，( $\color{red}1$ 上方的数为 $\color{blue}-c_{1j}, -c_{2j}, \cdots, -c_{(j-1)j}$, 下方的数字全都是 $\color{pink}0$ )
$$
{\color{blue}\beta_j}=\left[\begin{array}c \color{blue}\beta_1 & \color{blue}\cdots & \color{red}\alpha_j  & \color{red}\cdots & \color{red}\alpha_n \end{array}\right]
\left[\begin{array}c \color{blue}-c_{1j} \\ \color{blue}\vdots \\ \color{red}1 \\ \color{pink}\vdots \\ \color{pink}0 \end{array}\right]
$$
把它们写到一起，就得到了 $\gamma_{1j}$ 的表达式
$$
\left[\begin{array}c \color{blue}\beta_1 & \color{blue}\cdots & \color{blue}\beta_j & \color{red}\cdots & \color{red}\alpha_n\end{array}\right]=\left[\begin{array}c \color{blue}\beta_1 & \color{blue}\cdots & \color{red}\alpha_j  & \color{red}\cdots & \color{red}\alpha_n \end{array}\right]
\left[\begin{array}c
\color{blue}1 && \color{blue}-c_{1j}\\
& \color{blue}\ddots & \color{blue}\vdots\\
&& \color{red}1\\
&&& \color{red}\ddots\\
&&&& \color{red} 1
\end{array}\right]
$$
$\gamma_1$ 实际上就是把这些列操作合并到一起
$$
\beta=\alpha\gamma_{12}\gamma_{13}\cdots \gamma_{1n}\Rightarrow \gamma_1=\gamma_{12}\gamma_{13}\cdots \gamma_{1n}
$$
但是你会发现合并这些项很困难，原因和第2章的一样，解决方案也和第二章的一样：找 $\gamma$ 的逆，它会更直接告诉我们关于正交化的信息：

容易验证（读者应当自己验证）
$$
\gamma_{1j}^{-1}=\left[\begin{array}c
\color{blue}1 && \color{blue}c_{1j}\\
& \color{blue}\ddots & \color{blue}\vdots\\
&& \color{red}1\\
&&& \color{red}\ddots\\
&&&& \color{red} 1
\end{array}\right]
$$
其中对角线都是 $1$, 在第 $j$ 列主对角线上方第 $i$ 行的元素为 $\color{blue}c_{ij}$，其它所有元素都是 $0$. 把它们乘到一起得到了
$$
\gamma_{1}^{-1}=\gamma_{1n}^{-1}\gamma_{1(n-1)}^{-1}\cdots\gamma_{12}^{-1}=\left[\begin{array}c
1 & c_{12} & \cdots & c_{1n}\\
& 1 & \cdots & c_{2n}\\
&& \ddots & \vdots\\
&&& 1
\end{array}\right]
$$
这并没有结束，$\gamma_2$ 把 $\beta$ 中的每个向量除以它的模长让它们归一化，所以
$$
\gamma_2=\left[\begin{array}c
|\beta_1|^{-1} \\
& |\beta_2|^{-1} \\
&& \ddots \\
&&& |\beta_n|^{-1}
\end{array}\right]\Rightarrow
\gamma_2^{-1}=\left[\begin{array}c
|\beta_1| \\
& |\beta_2| \\
&& \ddots \\
&&& |\beta_n|
\end{array}\right]
$$
而 $\gamma^{-1}$ 的表达式
$$
\begin{aligned}
\varepsilon&=\beta\gamma_2=\alpha\gamma_1\gamma_2=\alpha\gamma\Rightarrow \gamma=\gamma_1\gamma_2\Rightarrow\\
\gamma^{-1}&=\gamma_2^{-1}\gamma_1^{-1}\\
&=\left[\begin{array}c
|\beta_1| \\
& |\beta_2| \\
&& \ddots \\
&&& |\beta_n|
\end{array}\right]\left[\begin{array}c
1 & c_{12} & \cdots & c_{1n}\\
& 1 & \cdots & c_{2n}\\
&& \ddots & \vdots\\
&&& 1
\end{array}\right]\\
&=\left[\begin{array}c
|\beta_1| & |\beta_1|c_{12} & \cdots & |\beta_1|c_{1n}\\
& |\beta_2| & \cdots & |\beta_2|c_{2n}\\
&& \ddots & \vdots\\
&&& |\beta_n|
\end{array}\right]
\end{aligned}
$$
在这么做的过程中 $\alpha=\varepsilon \gamma^{-1}$ 被分解为一个归一正交基和一个上三角矩阵，对于 $H=\mathbb{R}^n$ 且使用标准内积的情况，这把一个矩阵分解为两个 $M=QR$. 其中 $Q$ 中的向量是归一正交的，每列模长都是 $1$, 且每两个不同的列都垂直，这样的矩阵被称为正交阵.（ 这与 $Q^{\mathrm{T}}Q=I_n$ 等价）$R$ 是一个上三角矩阵，其中的系数表示出我们是如何操作向量组让它正交的.

例如, $\alpha=\left[\begin{array}c 1 & 1 & 0 \\ 0 & 1 & 1 \\ 1 & 0 & 1 \end{array}\right]$ 是 $\mathbb{R}^3$ 的一个基，但并非归一正交，让我们用施密特正交化方法把它正交化:

1. $\beta_1=\alpha_1=\left[\begin{array}c 1 \\ 0 \\ 1 \end{array}\right]$

   $\dfrac{\langle \beta_1, \alpha_2\rangle}{\langle \beta_1, \beta_1\rangle}=\dfrac12$, $\beta_2=\alpha_2-\dfrac12\beta_1=\left[\begin{array}c 1/2 \\ 1 \\ -1/2 \end{array}\right]$, 

   $\dfrac{\langle \beta_1, \alpha_3\rangle}{\langle \beta_1, \beta_1\rangle}=\dfrac12$, $\dfrac{\langle \beta_2, \alpha_3\rangle}{\langle \beta_2, \beta_2\rangle}=\dfrac13$, $\beta_3=\alpha_3-\dfrac12\beta_1-\dfrac13\beta_2=\left[\begin{array}c -2/3 \\ 2/3 \\ 2/3 \end{array}\right]$

2. $\varepsilon_1= |\beta_1|^{-1}\beta_1=\frac1{\sqrt2}\left[\begin{array}c 1 \\ 0 \\ 1 \end{array}\right]$

   $\varepsilon_2=|\beta_2|^{-1}\beta_2=\dfrac1{\sqrt6}\left[\begin{array}c 1 \\ 2 \\ -1\end{array}\right]$

   $\varepsilon_3=|\beta_3|^{-1}\beta_3=\dfrac1{\sqrt3}\left[\begin{array}c -1 \\ 1 \\ 1 \end{array}\right]$

基底 $\left[\begin{array}c \varepsilon_1 & \varepsilon_2 & \varepsilon_3 \end{array}\right]$ 归一正交，相应的分解出的上三角矩阵 $R$ 为
$$
R=\left[\begin{array}c \sqrt 2 \\ & \sqrt3/\sqrt2 \\ && 2/\sqrt3\end{array}\right]
\left[\begin{array}r 1 & -1/2 & -1/2 \\ & 1 & -1/3 \\ &&1 \end{array}\right]
$$

#### 一般空间上内积的例子

1. 区间 $[a, b]$ 上的连续函数构成向量空间，微积分告诉我们这样的函数总是可积的，可以定义两个函数的内积为
   $$
   \langle f, g\rangle=\int_a^b f(x)g(x)\mathrm{d} x
   $$
   注意到积分可以视作指标 $x$ 在连续集合内取值时的求和，此外我们也可以乘以一个连续函数 $\mu(x)$ 作为权重以得到 $[a, b]$ 上的加权内积
   $$
   \langle f, g\rangle = \int_a^b f(x)g(x)\mu(x)\mathrm{d} x
   $$
   这样定义的内积对泛函分析和信号处理等很有用

2. 在概率论中，两个随机变量的内积被定义为乘积的期望值，$\langle X, Y\rangle=\operatorname{E}(XY)$. 即使你不知道这是什么意思，它为什么成立也没关系，你只需要知道线性代数对概率论也很重要

3. 为了比较两幅图片或者两个离散信号的相似度，可以把它们视作向量. 内积可以反映它们有多相似.

#### 关于李群的简介 *

> 初学者如果不熟悉群可以跳过这一节
>
> 一个群是带有一个二元运算的集合 $(G, \cdot)$
>
> 1. 运算满足结合律
> 2. 存在单位元
> 3. 所有元素都存在逆

设 $Q\in \mathbb{R}^{n\times n}$ 是正交阵，则每一列模长都是 $1$ 而不同列正交，所以 $Q^{\mathrm{T}}Q=I_n$, 即 $Q^{-1}=Q^{\mathrm{T}}$. 在变换后两个向量的内积是
$$
(Qu)\cdot(Qv)=(Qu)^{\mathrm{T}}(Qv)=u^{\mathrm{T}}Q^{\mathrm{T}}Qv=u^{\mathrm{T}}I_n v=u^{\mathrm{T}}v=u\cdot v
$$
这表明内积在正交阵作用下保持不变，所以长度和角度也保持不变. 注意到 $e_i$ 的模是 $1$, 而 $e_i$ 和 $e_j$ 的点积是 $0$, 所以一个保持内积的矩阵的列模长总是 $1$ 并且互相垂直. 如果 $P,Q$ 都是正交矩阵，那么 $(PQu)\cdot (PQv)=(Qu)\cdot (Qv)=u\cdot v$, 所以 $PQ$ 也保持内积不变，是正交阵，故正交阵组成的集合在矩阵乘法下封闭，$n\times n$ 矩阵全体被称为 $\mathbb{R}^n$ 上的正交群, 记作 $O_n$, 或者 $O_n(\mathbb{R})$. 因为元素可以连续取值，所以这又被称作一个李群.





#### 练习

$4.1$ 对 $4$ 个向量 $\alpha_1=\left[\begin{array}c 2 \\ 1 \\ 2 \\ 1 \end{array}\right]$, $\alpha_2=\left[\begin{array}c 2 \\ 1 \\ -2 \\ -1 \end{array}\right]$, $\alpha_3=\left[\begin{array}c 1 \\ 2 \\ -1 \\ -2 \end{array}\right]$, $\alpha_4=\left[\begin{array}c 1 \\ 2 \\ 1 \\ 2 \end{array}\right]$. 计算两两之间的点积，哪些向量对是互相垂直的？

$4.2$ 先检验  $\alpha_1=\left[\begin{array}c 1 \\ -1 \\ 0 \end{array}\right]$, $\alpha_2=\left[\begin{array}c 0 \\ 1 \\ -1 \end{array}\right]$ 是 $\operatorname{Ker}\left[\begin{array}c 1 & 1 & 1\end{array}\right]$ 的一个基，然后计算这个基下的度量矩阵，设在这个基下， $v_1$ 坐标为 $x_1=\left[\begin{array}c 2 \\ 1 \end{array}\right]$, $v_2$ 坐标为 $x_2=\left[\begin{array}c 1 \\ 2 \end{array}\right]$, 计算 $v_1$ 和 $v_2$ 的模和夹角.

$4.3$ 设一个二维的向量空间在某基底 $\alpha$ 下有度量矩阵 $G=\left[\begin{array}c 3 & 1 \\ 1 & 3 \end{array}\right]$. 两个基向量的模长是多少？$\alpha_1$ 和 $\alpha_2$ 点的距离是多少？根据边长，计算三角形 $0\alpha_1\alpha_2$ 的面积（面积可以用长度和角度计算，所以面积也由内积决定）

<center><img src="images/4-1.svg"/></center>

$4.4$ 证明 $\alpha-\operatorname{proj}_{\beta}(\alpha)$ 和 $\beta$ 垂直，对多项式定义如下内积：$\displaystyle \langle f, g\rangle=\int_0^1 f(x)g(x)\mathrm{d} x$. 如果 $f(x)=1$ 是常值函数，$g(x)=x$, 计算 $g-\operatorname{proj}_{f}g$

$4.5$ (这一问需要一些计算能力) 假设对小于 $3$ 次的多项式(即$a+bx+cx^2$ 形式的多项式)定义如下内积：
$$
\displaystyle \langle f, g\rangle=\int_0^{+\infty} f(x)g(x)\mathrm{e}^{-x}\mathrm{d} x
$$
显然 $\alpha_1=1,\alpha_2=x,\alpha_3=x^2$ 是一个基，计算这个基下的度量矩阵并且用施密特正交化方法为这个空间找到一个归一正交基.

$4.6$ 注意到 $\alpha=\varepsilon R$ 把一个向量组 $\alpha$ 写成一个归一正交向量组 $\varepsilon$ 的线性组合，其中 $R$ 作用在右侧作为过渡矩阵. 为这个基变换过程画一个交换图. 如果某向量原来的坐标是 $x$, 它在归一正交基下的坐标是什么？

